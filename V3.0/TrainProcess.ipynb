{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "from skimage import io, transform\n",
    "from random import randint\n",
    "import shutil\n",
    "\n",
    "import os, uuid, glob, warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS FOR FLICKER LOGOS DATASET 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = '../data/fl27/original'\n",
    "CROPPED_DATA_DIR = '../data/fl27/images'\n",
    "ORIGINAL_ANNOTATION = '../data/fl27/annotation.txt'\n",
    "CROPPED_ANNOTATION = '../data/fl27/crop_annotation.txt'\n",
    "\n",
    "TRAIN_SET = '../annotations/trainset.txt'\n",
    "TEST_SET = '../annotations/testset.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS FOR FLICKER LOGOS DATASET 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = '../data/fl32/originals'\n",
    "CROPPED_DATA_DIR = '../data/fl32/images'\n",
    "ORIGINAL_ANNOTATION = '../data/fl32/annotation.txt'\n",
    "CROPPED_ANNOTATION = '../data/fl32/crop_annotation.txt'\n",
    "\n",
    "\n",
    "TRAIN_SET = '../annotations/trainset32.txt'\n",
    "TEST_SET = '../annotations/testset32.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_annotation(path):\n",
    "    file = open(path, \"r\")\n",
    "    content = file.readlines()\n",
    "    new = [x.split(\" \")[:-1] for x in content]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_file, root,transform=None):\n",
    "        self.txt_file = txt_file\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.txt_file.shape[0]\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        img_name = os.path.join(self.root, self.txt_file[id][0])\n",
    "        img = io.imread(img_name)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        logo = int(self.txt_file[id][1])\n",
    "        return img,logo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels for FL 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Adidas', 'Apple', 'BMW', 'Citroen', 'Cocacola', 'DHL', 'Fedex', 'Ferrari', \n",
    "          'Ford', 'Google', 'Heineken', 'HP', 'Intel', 'McDonalds', 'Mini', 'Nbc', 'Nike', 'Pepsi', \n",
    "          'Porsche', 'Puma', 'RedBull', 'Sprite', 'Starbucks', 'Texaco', 'Unicef', 'Vodafone', 'Yahoo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['ferrari', 'ups', 'cocacola', 'guiness', 'adidas', 'aldi', 'texaco', 'nvidia', 'rittersport', \n",
    "          'paulaner', 'dhl', 'bmw', 'fosters', 'milka', 'starbucks', 'pepsi', 'singha', 'apple', 'fedex',\n",
    "          'carlsberg', 'hp', 'chimay', 'google', 'tsingtao', 'corona', 'ford', 'esso', 'shell', 'stellaartois', \n",
    "          'becks', 'heineken', 'erdinger','nologo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(LABELS[20])\n",
    "print(len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_num_dataset(annotation_path, set_path):\n",
    "    arr = read_from_annotation(set_path)\n",
    "    out = []\n",
    "    for item in arr:\n",
    "        tmp = [item[0], LABELS.index(item[1].split('\\n')[0])]\n",
    "        out.append(tmp)\n",
    "    out = np.array(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 7404 items and test size 2000 items\n"
     ]
    }
   ],
   "source": [
    "train_data = prepare_num_dataset(CROPPED_ANNOTATION, TRAIN_SET)\n",
    "test_data = prepare_num_dataset(CROPPED_ANNOTATION, TEST_SET)\n",
    "trainset = MyDataset(train_data, CROPPED_DATA_DIR,transform)\n",
    "testset = MyDataset(test_data, CROPPED_DATA_DIR,transform)\n",
    "print(\"Train size {} items and test size {} items\".format(len(trainset), len(testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                           batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "\n",
    "n_classes = len(LABELS)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, n_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        print(x)\n",
    "        print(x.size(0))\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        print(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "cnn = CNN(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1513363039688/work/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7273561cb99b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1513363039688/work/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1513363039688/work/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-424e58459555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-5c494feaa886>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         )\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kamalkhan/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1513363039688/work/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr = learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = to_var(images)\n",
    "        labels = to_var(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n",
    "            %(epoch+1, num_epochs, i+1, len(trainset)//batch_size, loss.data[0]))\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn-fl32-vol3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 97 %\n",
      "Time for running: 24\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "cnn = CNN(n_classes)\n",
    "cnn.load_state_dict(torch.load('cnn-fl2-vol3.pt'))\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('Test Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n",
    "print('Time for running: %d' % (elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4ac228950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_file = 'cnn-vol-2-loss.txt' \n",
    "def read_from_loss(file):\n",
    "    with open (file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.split(' ') for x in content]\n",
    "        content = [[x[i].split('\\n')[0] for i in range(len(x)) if i%2!=0] for x in content]\n",
    "        content = [[x[0]+x[1] , x[2]] for x in content]\n",
    "    data = []\n",
    "    for item in content:\n",
    "        tmp = item[0].split(',')\n",
    "        num = \"\"\n",
    "        for t in tmp:\n",
    "            num += (t.split('/')[0].split('[')[1]) + '.'\n",
    "        iteration = float(num[:-1])\n",
    "        data.append([iteration, float(item[1])])\n",
    "    data = np.array(data)\n",
    "    plt.plot(data[: , 0], data[: , 1])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "read_from_loss(loss_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample testing of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selectivesearch\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGES_FOR_SEGMENTATION = '../data/fl27/segmentation/original'\n",
    "SEGMENTED_RESIZED_IMAGES = '../data/fl27/segmentation/segmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = glob.glob(os.path.join(TEST_IMAGES_FOR_SEGMENTATION,'*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in test_images:\n",
    "    im = io.imread(img)\n",
    "    img_lbl,regions = selectivesearch.selective_search(im, scale=500, sigma=0.9, min_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_regions = []\n",
    "for item in regions:\n",
    "    tmp = item['rect']\n",
    "    if tmp not in filtered_regions:\n",
    "        filtered_regions.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 156, 93), (110, 0, 389, 101), (287, 0, 212, 76), (437, 0, 48, 49), (100, 17, 269, 305), (181, 24, 161, 70), (68, 33, 158, 288), (124, 34, 224, 158), (0, 38, 171, 158), (337, 58, 162, 117), (0, 84, 93, 34), (108, 95, 35, 107), (392, 100, 107, 31), (111, 112, 247, 190), (221, 119, 71, 83), (120, 122, 63, 73), (0, 136, 227, 204), (343, 143, 156, 109), (372, 145, 86, 26), (311, 153, 80, 135), (0, 159, 499, 215), (269, 230, 230, 109), (188, 313, 92, 34), (181, 24, 318, 151), (100, 17, 399, 322), (110, 0, 389, 175), (124, 34, 375, 218), (0, 34, 499, 306), (100, 17, 399, 330), (0, 17, 499, 330), (0, 0, 171, 196), (0, 0, 183, 196), (110, 0, 389, 288), (0, 0, 499, 288), (0, 0, 499, 321), (0, 17, 499, 357), (0, 0, 499, 374)]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in filtered_regions:\n",
    "    rect = item\n",
    "    x1 = rect[0]\n",
    "    y1 = rect[1]\n",
    "    x2 = rect[2]\n",
    "    y2 = rect[3]\n",
    "    \n",
    "    if x1>x2:\n",
    "        tmp = x1\n",
    "        x1 = x2\n",
    "        x2 = tmp\n",
    "        \n",
    "    if y1>y2:\n",
    "        tmp = y1\n",
    "        y1 = y2\n",
    "        y2 = tmp\n",
    "    \n",
    "    new_img = im[x1:x2, y1:y2]\n",
    "    resized = resize(new_img,(224,224))\n",
    "    new_f_name = os.path.join(SEGMENTED_RESIZED_IMAGES,uuid.uuid4().hex+'.jpg')\n",
    "    \n",
    "    io.imsave(new_f_name, resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "def load_image(image, transform=None):\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn = CNN(n_classes)\n",
    "test_cnn.eval()\n",
    "test_cnn.load_state_dict(torch.load('cnn-vol-2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (3): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (6): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=27)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = io.imread(os.path.join(SEGMENTED_RESIZED_IMAGES,'991d371972114c9da5552238e61fa50f.jpg'))\n",
    "image = load_image(test_image, transform)\n",
    "image_tensor = to_var(image, volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicef\n"
     ]
    }
   ],
   "source": [
    "output = test_cnn(image_tensor)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(LABELS[int(predicted)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
