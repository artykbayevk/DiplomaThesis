{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "from skimage import io, transform\n",
    "from random import randint\n",
    "import shutil\n",
    "\n",
    "import os, uuid, glob, warnings\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS FOR FLICKER LOGOS DATASET 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = '../data/fl27/original'\n",
    "CROPPED_DATA_DIR = '../data/fl27/images'\n",
    "ORIGINAL_ANNOTATION = '../data/fl27/annotation.txt'\n",
    "CROPPED_ANNOTATION = '../data/fl27/crop_annotation.txt'\n",
    "\n",
    "TRAIN_SET = '../annotations/trainset.txt'\n",
    "TEST_SET = '../annotations/testset.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS FOR FLICKER LOGOS DATASET 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = '../data/fl32/originals'\n",
    "CROPPED_DATA_DIR = '../data/fl32/images'\n",
    "ORIGINAL_ANNOTATION = '../data/fl32/annotation.txt'\n",
    "CROPPED_ANNOTATION = '../data/fl32/crop_annotation.txt'\n",
    "\n",
    "\n",
    "TRAIN_SET = '../annotations/trainset32.txt'\n",
    "TEST_SET = '../annotations/testset32.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_annotation(path):\n",
    "    file = open(path, \"r\")\n",
    "    content = file.readlines()\n",
    "    new = [x.split(\" \")[:-1] for x in content]\n",
    "    return new\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_file, root,transform=None):\n",
    "        self.txt_file = txt_file\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.txt_file.shape[0]\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        img_name = os.path.join(self.root, self.txt_file[id][0])\n",
    "        img = Image.open(img_name)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        logo = int(self.txt_file[id][1])\n",
    "        return img,logo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels for FL 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Adidas', 'Apple', 'BMW', 'Citroen', 'Cocacola', 'DHL', 'Fedex', 'Ferrari', \n",
    "          'Ford', 'Google', 'Heineken', 'HP', 'Intel', 'McDonalds', 'Mini', 'Nbc', 'Nike', 'Pepsi', \n",
    "          'Porsche', 'Puma', 'RedBull', 'Sprite', 'Starbucks', 'Texaco', 'Unicef', 'Vodafone', 'Yahoo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "LABELS = ['ferrari', 'ups', 'cocacola', 'guiness', 'adidas', 'aldi', 'texaco', 'nvidia', 'rittersport', \n",
    "          'paulaner', 'dhl', 'bmw', 'fosters', 'milka', 'starbucks', 'pepsi', 'singha', 'apple', 'fedex',\n",
    "          'carlsberg', 'hp', 'chimay', 'google', 'tsingtao', 'corona', 'ford', 'esso', 'shell', 'stellaartois', \n",
    "          'becks', 'heineken', 'erdinger','nologo']\n",
    "print(LABELS[20])\n",
    "print(len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 7404 items and test size 2000 items\n",
      "Batch size: 40\n"
     ]
    }
   ],
   "source": [
    "def prepare_num_dataset(annotation_path, set_path):\n",
    "    arr = read_from_annotation(set_path)\n",
    "    out = []\n",
    "    for item in arr:\n",
    "        tmp = [item[0], LABELS.index(item[1].split('\\n')[0])]\n",
    "        out.append(tmp)\n",
    "    out = np.array(out)\n",
    "    return out\n",
    "\n",
    "train_data = prepare_num_dataset(CROPPED_ANNOTATION, TRAIN_SET)\n",
    "test_data = prepare_num_dataset(CROPPED_ANNOTATION, TEST_SET)\n",
    "trainset = MyDataset(train_data, CROPPED_DATA_DIR,transform)\n",
    "testset = MyDataset(test_data, CROPPED_DATA_DIR,transform)\n",
    "print(\"Train size {} items and test size {} items\".format(len(trainset), len(testset)))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                           batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "print('Batch size: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "n_classes = len(LABELS)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, n_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "cnn = CNN(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (3): Conv2d (64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (6): Conv2d (192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d (384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=33)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] Loss: 1.2137\n",
      "Epoch [20/1000] Loss: 0.8017\n",
      "Epoch [30/1000] Loss: 1.3360\n",
      "Epoch [40/1000] Loss: 0.0828\n",
      "Epoch [50/1000] Loss: 1.8922\n",
      "Epoch [60/1000] Loss: 0.4025\n",
      "Epoch [70/1000] Loss: 1.0520\n",
      "Epoch [80/1000] Loss: 3.1454\n",
      "Epoch [90/1000] Loss: 2.0841\n",
      "Epoch [100/1000] Loss: 2.4527\n",
      "Epoch [110/1000] Loss: 1.8642\n",
      "Epoch [120/1000] Loss: 1.9373\n",
      "Epoch [130/1000] Loss: 1.8800\n",
      "Epoch [140/1000] Loss: 0.9070\n",
      "Epoch [150/1000] Loss: 2.0418\n",
      "Epoch [160/1000] Loss: 0.0663\n",
      "Epoch [170/1000] Loss: 1.8703\n",
      "Epoch [180/1000] Loss: 0.3652\n",
      "Epoch [190/1000] Loss: 1.0882\n",
      "Epoch [200/1000] Loss: 2.0079\n",
      "Epoch [210/1000] Loss: 1.8676\n",
      "Epoch [220/1000] Loss: 3.8519\n",
      "Epoch [230/1000] Loss: 0.2348\n",
      "Epoch [240/1000] Loss: 2.0494\n",
      "Epoch [250/1000] Loss: 2.8304\n",
      "Epoch [260/1000] Loss: 2.9224\n",
      "Epoch [270/1000] Loss: 1.8333\n",
      "Epoch [280/1000] Loss: 1.8038\n",
      "Epoch [290/1000] Loss: 0.8949\n",
      "Epoch [300/1000] Loss: 1.6371\n",
      "Epoch [310/1000] Loss: 0.9846\n",
      "Epoch [320/1000] Loss: 1.8021\n",
      "Epoch [330/1000] Loss: 1.6703\n",
      "Epoch [340/1000] Loss: 1.7375\n",
      "Epoch [350/1000] Loss: 0.7770\n",
      "Epoch [360/1000] Loss: 1.3747\n",
      "Epoch [370/1000] Loss: 2.8974\n",
      "Epoch [380/1000] Loss: 2.5804\n",
      "Epoch [390/1000] Loss: 1.5964\n",
      "Epoch [400/1000] Loss: 1.5522\n",
      "Epoch [410/1000] Loss: 1.5217\n",
      "Epoch [420/1000] Loss: 2.4434\n",
      "Epoch [430/1000] Loss: 0.0253\n",
      "Epoch [440/1000] Loss: 1.8447\n",
      "Epoch [450/1000] Loss: 2.0080\n",
      "Epoch [460/1000] Loss: 0.3099\n",
      "Epoch [470/1000] Loss: 1.1120\n",
      "Epoch [480/1000] Loss: 0.7444\n",
      "Epoch [490/1000] Loss: 1.5570\n",
      "Epoch [500/1000] Loss: 0.0540\n",
      "Epoch [510/1000] Loss: 2.0176\n",
      "Epoch [520/1000] Loss: 0.0208\n",
      "Epoch [530/1000] Loss: 0.1412\n",
      "Epoch [540/1000] Loss: 0.3934\n",
      "Epoch [550/1000] Loss: 1.1700\n",
      "Epoch [560/1000] Loss: 0.5456\n",
      "Epoch [570/1000] Loss: 0.3269\n",
      "Epoch [580/1000] Loss: 0.0039\n",
      "Epoch [590/1000] Loss: 0.5473\n",
      "Epoch [600/1000] Loss: 0.7780\n",
      "Epoch [610/1000] Loss: 0.8032\n",
      "Epoch [620/1000] Loss: 0.8498\n",
      "Epoch [630/1000] Loss: 0.5895\n",
      "Epoch [640/1000] Loss: 0.0103\n",
      "Epoch [650/1000] Loss: 0.0021\n",
      "Epoch [660/1000] Loss: 0.0245\n",
      "Epoch [670/1000] Loss: 2.8428\n",
      "Epoch [680/1000] Loss: 3.7277\n",
      "Epoch [690/1000] Loss: 0.2834\n",
      "Epoch [700/1000] Loss: 0.2572\n",
      "Epoch [710/1000] Loss: 0.8065\n",
      "Epoch [720/1000] Loss: 0.7096\n",
      "Epoch [730/1000] Loss: 1.1532\n",
      "Epoch [740/1000] Loss: 0.3856\n",
      "Epoch [750/1000] Loss: 0.6092\n",
      "Epoch [760/1000] Loss: 0.0528\n",
      "Epoch [770/1000] Loss: 0.0081\n",
      "Epoch [780/1000] Loss: 0.3930\n",
      "Epoch [790/1000] Loss: 0.4104\n",
      "Epoch [800/1000] Loss: 1.3757\n",
      "Epoch [810/1000] Loss: 0.3140\n",
      "Epoch [820/1000] Loss: 0.2478\n",
      "Epoch [830/1000] Loss: 0.1060\n",
      "Epoch [840/1000] Loss: 0.3628\n",
      "Epoch [850/1000] Loss: 0.0347\n",
      "Epoch [860/1000] Loss: 0.9734\n",
      "Epoch [870/1000] Loss: 0.1789\n",
      "Epoch [880/1000] Loss: 0.1574\n",
      "Epoch [890/1000] Loss: 0.9401\n",
      "Epoch [900/1000] Loss: 0.0028\n",
      "Epoch [910/1000] Loss: 0.3314\n",
      "Epoch [920/1000] Loss: 0.0483\n",
      "Epoch [930/1000] Loss: 0.0014\n",
      "Epoch [940/1000] Loss: 0.7632\n",
      "Epoch [950/1000] Loss: 0.8265\n",
      "Epoch [960/1000] Loss: 0.0002\n",
      "Epoch [970/1000] Loss: 1.0485\n",
      "Epoch [980/1000] Loss: 1.5268\n",
      "Epoch [990/1000] Loss: 0.3389\n",
      "Epoch [1000/1000] Loss: 0.1426\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr = learning_rate, momentum=momentum)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = to_var(images)\n",
    "        labels = to_var(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print ('Epoch [%d/%d] Loss: %.4f'\n",
    "            %(epoch+1, num_epochs, loss.data[0]))\n",
    "        torch.save(cnn.state_dict(), 'cnn-fl22-vol3-{}.pt'.format(epoch+1))\n",
    "\n",
    "# Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 78 %\n",
      "Time for running: 67\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "cnn = CNN(n_classes)\n",
    "cnn.load_state_dict(torch.load('cnn-fl22-vol3-960.pt'))\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('Test Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n",
    "print('Time for running: %d' % (elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_file = 'cnn-vol-2-loss.txt' \n",
    "def read_from_loss(file):\n",
    "    with open (file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.split(' ') for x in content]\n",
    "        content = [[x[i].split('\\n')[0] for i in range(len(x)) if i%2!=0] for x in content]\n",
    "        content = [[x[0]+x[1] , x[2]] for x in content]\n",
    "    data = []\n",
    "    for item in content:\n",
    "        tmp = item[0].split(',')\n",
    "        num = \"\"\n",
    "        for t in tmp:\n",
    "            num += (t.split('/')[0].split('[')[1]) + '.'\n",
    "        iteration = float(num[:-1])\n",
    "        data.append([iteration, float(item[1])])\n",
    "    data = np.array(data)\n",
    "    plt.plot(data[: , 0], data[: , 1])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "read_from_loss(loss_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample testing of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selectivesearch\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGES_FOR_SEGMENTATION = '../data/fl27/segmentation/original'\n",
    "SEGMENTED_RESIZED_IMAGES = '../data/fl27/segmentation/segmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = glob.glob(os.path.join(TEST_IMAGES_FOR_SEGMENTATION,'*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in test_images:\n",
    "    im = io.imread(img)\n",
    "    img_lbl,regions = selectivesearch.selective_search(im, scale=500, sigma=0.9, min_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_regions = []\n",
    "for item in regions:\n",
    "    tmp = item['rect']\n",
    "    if tmp not in filtered_regions:\n",
    "        filtered_regions.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in filtered_regions:\n",
    "    rect = item\n",
    "    x1 = rect[0]\n",
    "    y1 = rect[1]\n",
    "    x2 = rect[2]\n",
    "    y2 = rect[3]\n",
    "    \n",
    "    if x1>x2:\n",
    "        tmp = x1\n",
    "        x1 = x2\n",
    "        x2 = tmp\n",
    "        \n",
    "    if y1>y2:\n",
    "        tmp = y1\n",
    "        y1 = y2\n",
    "        y2 = tmp\n",
    "    \n",
    "    new_img = im[x1:x2, y1:y2]\n",
    "    resized = resize(new_img,(224,224))\n",
    "    new_f_name = os.path.join(SEGMENTED_RESIZED_IMAGES,uuid.uuid4().hex+'.jpg')\n",
    "    \n",
    "    io.imsave(new_f_name, resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "def load_image(image, transform=None):\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn = CNN(n_classes)\n",
    "test_cnn.eval()\n",
    "test_cnn.load_state_dict(torch.load('cnn-vol-2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = io.imread(os.path.join(SEGMENTED_RESIZED_IMAGES,'991d371972114c9da5552238e61fa50f.jpg'))\n",
    "image = load_image(test_image, transform)\n",
    "image_tensor = to_var(image, volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test_cnn(image_tensor)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(LABELS[int(predicted)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
