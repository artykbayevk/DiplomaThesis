{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "from skimage import io, transform\n",
    "from random import randint\n",
    "import shutil\n",
    "\n",
    "import os, uuid, glob, warnings\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS FOR FLICKER LOGOS DATASET 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = '../data/fl27/original'\n",
    "CROPPED_DATA_DIR = '../data/fl27/images'\n",
    "ORIGINAL_ANNOTATION = '../data/fl27/annotation.txt'\n",
    "CROPPED_ANNOTATION = '../data/fl27/crop_annotation.txt'\n",
    "\n",
    "TRAIN_SET = '../annotations/trainset.txt'\n",
    "TEST_SET = '../annotations/testset.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS FOR FLICKER LOGOS DATASET 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = '../data/fl32/originals'\n",
    "CROPPED_DATA_DIR = '../data/fl32/images'\n",
    "ORIGINAL_ANNOTATION = '../data/fl32/annotation.txt'\n",
    "CROPPED_ANNOTATION = '../data/fl32/crop_annotation.txt'\n",
    "\n",
    "\n",
    "TRAIN_SET = '../annotations/trainset32.txt'\n",
    "TEST_SET = '../annotations/testset32.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_annotation(path):\n",
    "    file = open(path, \"r\")\n",
    "    content = file.readlines()\n",
    "    new = [x.split(\" \")[:-1] for x in content]\n",
    "    return new\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_file, root,transform=None):\n",
    "        self.txt_file = txt_file\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.txt_file.shape[0]\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        img_name = os.path.join(self.root, self.txt_file[id][0])\n",
    "        img = Image.open(img_name)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        logo = int(self.txt_file[id][1])\n",
    "        return img,logo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels for FL 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Adidas', 'Apple', 'BMW', 'Citroen', 'Cocacola', 'DHL', 'Fedex', 'Ferrari', \n",
    "          'Ford', 'Google', 'Heineken', 'HP', 'Intel', 'McDonalds', 'Mini', 'Nbc', 'Nike', 'Pepsi', \n",
    "          'Porsche', 'Puma', 'RedBull', 'Sprite', 'Starbucks', 'Texaco', 'Unicef', 'Vodafone', 'Yahoo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nologo\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "LABELS = ['ferrari', 'ups', 'cocacola', 'guiness', 'adidas', 'aldi', 'texaco', 'nvidia', 'rittersport', \n",
    "          'paulaner', 'dhl', 'bmw', 'fosters', 'milka', 'starbucks', 'pepsi', 'singha', 'apple', 'fedex',\n",
    "          'carlsberg', 'hp', 'chimay', 'google', 'tsingtao', 'corona', 'ford', 'esso', 'shell', 'stellaartois', \n",
    "          'becks', 'heineken', 'erdinger','nologo']\n",
    "print(LABELS[32])\n",
    "print(len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 8404 items and test size 1000 items\n",
      "Batch size: 15\n"
     ]
    }
   ],
   "source": [
    "def prepare_num_dataset(annotation_path, set_path):\n",
    "    arr = read_from_annotation(set_path)\n",
    "    out = []\n",
    "    for item in arr:\n",
    "        tmp = [item[0], LABELS.index(item[1].split('\\n')[0])]\n",
    "        out.append(tmp)\n",
    "    out = np.array(out)\n",
    "    return out\n",
    "\n",
    "train_data = prepare_num_dataset(CROPPED_ANNOTATION, TRAIN_SET)\n",
    "test_data = prepare_num_dataset(CROPPED_ANNOTATION, TEST_SET)\n",
    "trainset = MyDataset(train_data, CROPPED_DATA_DIR,transform)\n",
    "testset = MyDataset(test_data, CROPPED_DATA_DIR,transform)\n",
    "print(\"Train size {} items and test size {} items\".format(len(trainset), len(testset)))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 70\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                           batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "print('Batch size: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "n_classes = len(LABELS)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, n_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "cnn = CNN(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN (\n",
       "  (features): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU (inplace)\n",
       "    (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU (inplace)\n",
       "    (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU (inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU (inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU (inplace)\n",
       "    (12): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Dropout (p = 0.5)\n",
       "    (1): Linear (9216 -> 4096)\n",
       "    (2): ReLU (inplace)\n",
       "    (3): Dropout (p = 0.5)\n",
       "    (4): Linear (4096 -> 4096)\n",
       "    (5): ReLU (inplace)\n",
       "    (6): Linear (4096 -> 33)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500] Loss: 3.1264\n",
      "Epoch [20/500] Loss: 1.6110\n",
      "Epoch [30/500] Loss: 2.2609\n",
      "Epoch [40/500] Loss: 1.9291\n",
      "Epoch [50/500] Loss: 1.9732\n",
      "Epoch [60/500] Loss: 0.7036\n",
      "Epoch [70/500] Loss: 1.1888\n",
      "Epoch [80/500] Loss: 2.9764\n",
      "Epoch [90/500] Loss: 1.1771\n",
      "Epoch [100/500] Loss: 0.4418\n",
      "Epoch [110/500] Loss: 0.0063\n",
      "Epoch [120/500] Loss: 0.0538\n",
      "Epoch [130/500] Loss: 0.8441\n",
      "Epoch [140/500] Loss: 1.8677\n",
      "Epoch [150/500] Loss: 0.0279\n",
      "Epoch [160/500] Loss: 2.1514\n",
      "Epoch [170/500] Loss: 0.7830\n",
      "Epoch [180/500] Loss: 0.0278\n",
      "Epoch [190/500] Loss: 0.8292\n",
      "Epoch [200/500] Loss: 0.1652\n",
      "Epoch [210/500] Loss: 0.7431\n",
      "Epoch [220/500] Loss: 0.0810\n",
      "Epoch [230/500] Loss: 0.4506\n",
      "Epoch [240/500] Loss: 0.0820\n",
      "Epoch [250/500] Loss: 0.4758\n",
      "Epoch [260/500] Loss: 0.0018\n",
      "Epoch [270/500] Loss: 0.1659\n",
      "Epoch [280/500] Loss: 1.1016\n",
      "Epoch [290/500] Loss: 1.6601\n",
      "Epoch [300/500] Loss: 0.0156\n",
      "Epoch [310/500] Loss: 0.0038\n",
      "Epoch [320/500] Loss: 0.0099\n",
      "Epoch [330/500] Loss: 1.7033\n",
      "Epoch [340/500] Loss: 0.8691\n",
      "Epoch [350/500] Loss: 0.0337\n",
      "Epoch [360/500] Loss: 0.6209\n",
      "Epoch [370/500] Loss: 0.7725\n",
      "Epoch [380/500] Loss: 1.0288\n",
      "Epoch [390/500] Loss: 0.0015\n",
      "Epoch [400/500] Loss: 0.0002\n",
      "Epoch [410/500] Loss: 0.4926\n",
      "Epoch [420/500] Loss: 0.0012\n",
      "Epoch [430/500] Loss: 0.1262\n",
      "Epoch [440/500] Loss: 1.4011\n",
      "Epoch [450/500] Loss: 0.0137\n",
      "Epoch [460/500] Loss: 0.2102\n",
      "Epoch [470/500] Loss: 0.0004\n",
      "Epoch [480/500] Loss: 0.0000\n",
      "Epoch [490/500] Loss: 0.0048\n",
      "Epoch [500/500] Loss: 0.0044\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr = learning_rate, momentum=momentum)\n",
    "for epoch in range(num_epochs):\n",
    "    cnn.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = to_var(images)\n",
    "        labels = to_var(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print ('Epoch [%d/%d] Loss: %.4f'\n",
    "            %(epoch+1, num_epochs, loss.data[0]))\n",
    "#         torch.save(cnn.state_dict(), 'cnn-fl22-vol3-2nd-{}.pt'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(), 'model-lr.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 0 %\n",
      "Time for running: 55\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "cnn = CNN(n_classes)\n",
    "cnn.load_state_dict(torch.load('model-lr.pt'))\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('Test Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n",
    "print('Time for running: %d' % (elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_file = 'cnn-vol-2-loss.txt' \n",
    "def read_from_loss(file):\n",
    "    with open (file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.split(' ') for x in content]\n",
    "        content = [[x[i].split('\\n')[0] for i in range(len(x)) if i%2!=0] for x in content]\n",
    "        content = [[x[0]+x[1] , x[2]] for x in content]\n",
    "    data = []\n",
    "    for item in content:\n",
    "        tmp = item[0].split(',')\n",
    "        num = \"\"\n",
    "        for t in tmp:\n",
    "            num += (t.split('/')[0].split('[')[1]) + '.'\n",
    "        iteration = float(num[:-1])\n",
    "        data.append([iteration, float(item[1])])\n",
    "    data = np.array(data)\n",
    "    plt.plot(data[: , 0], data[: , 1])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "read_from_loss(loss_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUTURE TRAINING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "mnist = Mnist(batch_size)\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    capsule_net.train()\n",
    "    train_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(mnist.train_loader):\n",
    "\n",
    "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            print \"train accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n",
    "        \n",
    "    print train_loss / len(mnist.train_loader)\n",
    "        \n",
    "    capsule_net.eval()\n",
    "    test_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(mnist.test_loader):\n",
    "\n",
    "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "\n",
    "        test_loss += loss.data[0]\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            print \"test accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n",
    "    \n",
    "    print test_loss / len(mnist.test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample testing of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selectivesearch\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGES_FOR_SEGMENTATION = '../data/fl27/segmentation/original'\n",
    "SEGMENTED_RESIZED_IMAGES = '../data/fl27/segmentation/segmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = glob.glob(os.path.join(TEST_IMAGES_FOR_SEGMENTATION,'*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in test_images:\n",
    "    im = io.imread(img)\n",
    "    img_lbl,regions = selectivesearch.selective_search(im, scale=500, sigma=0.9, min_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_regions = []\n",
    "for item in regions:\n",
    "    tmp = item['rect']\n",
    "    if tmp not in filtered_regions:\n",
    "        filtered_regions.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in filtered_regions:\n",
    "    rect = item\n",
    "    x1 = rect[0]\n",
    "    y1 = rect[1]\n",
    "    x2 = rect[2]\n",
    "    y2 = rect[3]\n",
    "    \n",
    "    if x1>x2:\n",
    "        tmp = x1\n",
    "        x1 = x2\n",
    "        x2 = tmp\n",
    "        \n",
    "    if y1>y2:\n",
    "        tmp = y1\n",
    "        y1 = y2\n",
    "        y2 = tmp\n",
    "    \n",
    "    new_img = im[x1:x2, y1:y2]\n",
    "    resized = resize(new_img,(224,224))\n",
    "    new_f_name = os.path.join(SEGMENTED_RESIZED_IMAGES,uuid.uuid4().hex+'.jpg')\n",
    "    \n",
    "    io.imsave(new_f_name, resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "def load_image(image, transform=None):\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn = CNN(n_classes)\n",
    "test_cnn.eval()\n",
    "test_cnn.load_state_dict(torch.load('cnn-vol-2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = io.imread(os.path.join(SEGMENTED_RESIZED_IMAGES,'991d371972114c9da5552238e61fa50f.jpg'))\n",
    "image = load_image(test_image, transform)\n",
    "image_tensor = to_var(image, volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test_cnn(image_tensor)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(LABELS[int(predicted)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
