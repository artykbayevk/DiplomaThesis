\chapter{Review of Deep Learning and Pattern Recognition Algorithms}\label{ch:C}


\section{Introduction}\label{sec:3.1}
\par This chapter provides a review of the well-known algorithms of image segmentation, pattern recognition, exhaustive search and deep learning methods.We will also explain how the main image segmentation methods work and how they developed in computer vision. Also will demonstrated methods forward and backpropagation. Between this two process, you can see the optimization process, which try to minimize function of error. 


\section{Computer Vision and Pattern Recognition} \label{sec:3.2}
\par For a person, the perception of the outside world with your own eyes is a very simple task, be looking at any 2- or 3-dimensional object, you can safely tell about its shape and external structure.Looking at the crowd of people, the human brain can easily calculate the number of objects, can tell about their shape and condition. But what about the computer? Will the computer be able to handle the processing of objects that people see? Will the computer be able to find the difference between very similar objects? In this matter will help discipline called computer vision.This area is very closely related to signal processing, image processing, and video recording. As well as it includes machine learning with pattern recognition.Along with other Sciences like text processing and audio processing, science tries to create the ideal artificial intelligence that can think and act like a human.Image processing not only includes the transformation of images into a more comfortable and desired type but also this area along with computer vision will be able to show what is inside the image. Image processing not only includes the transformation of images into a more comfortable and desired look but also this area along with computer vision will be able to show what is inside the image. Also, this area helps in capturing movements inside the picture.[CVPR]
Understanding what exactly is happening on images and perception of this process is an important process in AI. Draw conclusions depending on what you see is a fairly simple process for a person, but not for the computer. Since a computer without any reason cannot understand the essence of the process. The problem in object recognition is the appearance of these objects in new forms or compositions because the pre-built model cannot cope with it, because it has not seen the object in this format. These new formats can be represented as an object in the expanded state, or it can be simply in motion. A huge number of new forms and aspects makes the object recognition a practically impossible task.[introTOCV]
%\vspace{-0.5cm}
%\par 

%\vspace{-0.5cm}
%\noindent 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Image Segmentation Methods}\label{sec:3.4}
\par In practice, the importance and value is not always fully the image itself, namely what are the specific parts of the image, and sometimes just the number of channels of the image.The first and one of the most important technologies for understanding what is happening inside this image is segmentation.Since only a segmentation can be divided into important and different parts.After all, it helps to understand the image inside the image, as well as to extract useful information for us.  These aspects are extremely important for programs where image recognition is paramount.For all these reasons, it can be understood that segmentation is a very important discipline within computer vision, and in turn, segmentation has a huge number of difficulties in implementing many methods.In short, segmentation is important for recognition, because it can pull out those areas that are very important for humans. And are the basis for all methods of recognition of contours and objects.There are many types of segmentation and a huge number of places where you can use them.One of the most common methods is threshold segmentation.The basis of this algorithm creates a segmentation of the image by its regions.This method searches for a threshold by a specific criterion to create a grayscale that will be distributed from other colors. This method sets a specific threshold for pixels and depending on the condition they change from 0 to 255 in grayscale.You can also mark a method called edge segmentation. This method is particularly the fact that he refers to the saturation of gray on the borders of any object.In the discipline of computer vision and related industries, there is no single segmentation method that can work in all cases. To use the segmentation method correctly, you need to consider the advantages and disadvantages. After all, each method will lead in different ways depending on the situation and the state in the image. And it is also very important to apply the correct parameters of segmentation methods. Since the parameters play a significant role in the algorithm.[1707.02051]

\par  Segmentation, by itself, is splitting the image into several areas, depending on their structure, size, and saturation of any particular colors. These areas can include grouped pixels, which represent the object itself, and can represent a variety of shapes, such as an arc, circle, or just a line. Developed regions can be simple lines or full-fledged objects that can have boundaries separating them from other content. Since the area of interest may not cover the entire image, we are interested in using segmentation in such cases. Segmentation has two main goals that it pursues. The first is to expand the image to the desired regions. The second task is to change the representation.Considering the simplest cases, when the interesting part of the image is very different from the rest, the segmentation will not be a problem. After all, the area of our interest, especially its color and saturation help to clearly separate it from the rest of the image. After all, the rest of the area does not have similar components as in the desired image area.But there are also severe cases where the boundaries are strongly distorted and erased as the color saturation is very similar, and the components do not differ from each other.Considering the second objective pursued by the segmentation can be sure it will ultimately give us a richer and more precise representation of the object within the image. Here our task is to gather pixels into one whole, into a more integral area, which is much useful and important for future research, because we create a clearer outline of the object. The perspective of an image can serve both as a useful tool and a very strong drawback since the borders can be clearly highlighted or even erased in the image. \textbf{Here will be one two images, seg1.png and seg2.png}Typically, classic segmentation techniques may not work well for images where the boundaries between the desired features are blurred and blurred, making this work practically impossible because the pixels are too similar and the features cannot be separated or isolated.To divide the image into several parts according to the regions, have to be extremely homogeneous as the level of gray. After all, black-and-white images are easier to work with due to algorithms. As well as the color and texture of the image are also important when dividing by regions. Neighboring areas of the desired object should have very different characteristics and features because the uniformity prevents the algorithm. Also, borders of the object should be evenly distributed, and also they should not be torn or distorted. Achieving all the above characteristics gives a certain amount of difficulty, after all, how would the objects did not have their uniform or completely, they still have dire and slits, which interfere with segmentation algorithms, making a homogeneous region in a heterogeneous region. Also, our eye can also be mistaken in terms of the homogeneity of the object inside the image, because sometimes there may be holes or cuts that are not subject to our eye, so the number of pixels that we can not see, can interfere with the segmentation.[ch10]


\subsection{Thresholding}\label{sec:3.4.1}
\par In the methods of segmentation are the segmentation types of image in parallel. The most common and easiest method is to segment an image using a threshold. This method is based on the use of gray color. After all, we know that translating the image into a black and white contour with it is more convenient to work with than 3 color channels. This method segments the image based on image separation by saturation and grayscale. It is able to divide image according to its local threshold, which is automatic, depending on the distribution of the white and black color. And also, you can split the image using a global defect which can be defined as static and automatic and manually. It can also be noted that the threshold can be dynamic because it changes from area to area by an image.Global threshold divides the image into the desired area and its background, which he considered not similar to the area of interest to us. The local one does this by going through the image, and depending on the situation and position, select a threshold to be divided into the main part and the background part.THE most common and convenient method of threshold segmentation is - Otsu method. This method uses the interclass variance to separate areas of an image. The method is special and distinctive in that it selects only the global threshold. And the threshold is chosen by the maximum dispersion between all classes inside the image. The segmentation method has found extensive application due to the fact that it is very simple to calculate and does not require costly calculations and calculation when the algorithm itself. Also, due to a simple calculation and increases the speed of the algorithm. This algorithm can work very well when the boundaries between the object and the background are separated by an accurate and bold contrast line. In such cases, you can obtain accurate segmentation results for the image. But in the opposite case, when the boundaries are erased, this method is not able to cope, because it will not know exactly what is the object and its background. Noise can easily interfere with this method. Because the noise erases the boundaries. And uniformity also can ruin the quality of the method is the segmentation threshold.This method is effective in combined use with other methods.[1707.02051]


\subsection{Clustering Methods}\label{sec:3.4.2}
\par  Clustering is a very powerful and unpredictable method in machine learning and computer vision. In computer vision, or rather in segmentation, it works by splitting image vectors into groups called clusters. Since clustering methods are not the same type, we can consider several types of clustering methods, but the essence of its work is based on similar points, which are very similar, and then they are grouped into separate clusters.The main problem of clustering is the correct splitting of the image into the correct sets of vectors. In this case, these vectors must be collected to have a similar value in the numbers, which means their structure must be similar. In these vectors consists mainly of the pixels of the image. Also, components can be indicators of the intensity in a given area, and 3 channel parameters that are related to each other. Texture, namely their calculated values can also be components. To associate pixels in groups, you can use any component or parameter that combines these pixels into a single value. Due to this, it is possible to find the associated objects and re-create the segmentation for the pixel count.[ch10]The\textit{ least squares error} is one of the most common measures to compare clusters that use the traditional method to break into groups. Clustering involves the process that determines the number of clusters $\kappa$. the same is created the number of groups from $C_{1}$ to $C_{\kappa}$. Each such cluster has its own personal measure of average $m_{\kappa}$. The formula of the error, said earlier, is calculated as follows:
\begin{equation}
D = {\sum\limits_{k=1}^{\kappa}} {\sum\limits_{x_i \in{C_{\kappa}}}^{\i}}{\Vert x_i - m_k \Vert}^{2}
\end{equation}

\par This formula shows how close this object's data is to a particular cluster. This procedure will help you to see all the possible options for partitioning into K-th number of clusters. As a result, it will find the best option to minimize our error function D. the Disadvantage of this method is that it is impossible to calculate everything. For this reason, they find the closest number in value and divide the rest of the objects into clusters. It is also very difficult to find a global and optimal variant of the error function iteratively, for this reason, they usually resort to the random selection of clusters and selection of their mean values for further calculations. This method is called k-means clustering. There is also a method that is different from it. It is called isodata clustering. It uses a similar method of splitting and merging.This method is based on creating groups from their distance from the center of a particular cluster. Clusters are initialized randomly and iteratively go through the positions to find the most optimal point at which the error function will be reaching.[ch10]

\par This method is the simplest and fastest, and most efficient for large datasets. Because it's easy to scale, it's very responsive for large datasets. The iterative nature of this method makes the optimization process easier and more convenient for calculations. But it also has a number of drawbacks, such as the number of clusters and the parameters by which these clusters need to be calculated. The iterative method is bad because every step goes through the whole sample, which is very expensive and time-consuming to calculate. There is also a problem with non-convex clusters, as they are difficult and impossible to calculate.[1707.02051]

\subsection{Edge detection}\label{sec:3.4.3}
\par  The edge segmentation method has its own feature to work faster due to the lack of a large amount of information that needs to be processed somehow because this method uses only the detected edges, which store all the necessary information for this method.Because of the very easy and understandable implementation, it is included along with other segmentation algorithms for recognition of an object. It has good ability to work in big data processing.The main advantage of this method we can call the extraction of accurate edge lines, which in the future will help to create their own borders and segment the image. Also when removing it takes out edge with the desired orientation. But we can not say for sure that the performance and reliability of this method are good or bad, because of the formulation of problems and the benefits of the method, each researcher must judge himself, based on the results that shows this method.This working method transforms the original image into divided regions which are clearly separated by edges in which the grey tone prevails. The edges that are at work of the algorithm become the boundaries between the object and its background, but again only the researcher can judge what is the background and what is the desired object. The benefit and contribution to the computer vision of this method are simply colossal. Also, a feature of this method in the creation of localization in the image of dependence on the level of gray color in the certain region.What are these edges? This is basically a change from pixel to pixel intensity of any color depending on a certain direction. From the same edges are extracted the main objects that will be used in the future. Because of the importance of grey, exploring the gaps between them is very important. Breaks can represent points, lines, and edges as well. Since breaks are useful, and they have based the basic methods of segmentation at the edges. It is possible to allocate methods such as Roberts, Sobel and Kirsh edge detection methods.[1211csit20]


\par  For the definition process, we can know for sure that not all points in the gradient have nonzero values. But at this point, we can say that not all points are important to us.During the improvement process, special pixels are highlighted. Under the peculiarity, we can say about the gradient change and about the intensity in this region. Also, to restore the image and get rid of unnecessary noise, we can recreate the filtering process.For example, we can talk about its two most common methods like Sobel operator and Laplacian of Gaussian Operator.\textbf{Sobel Operator:}
It goes through all gradients and highlights the rich and intense frequencies that pass through the object boundaries in the image.
\begin{table}[!htb]
	\caption{\textbf{Sobel Operator}}
	\begin{minipage}{.5\linewidth}
		\centering
		\begin{tabular}{|l|l|l|} \hline
			1 & 2 & 1 \\ \hline
			0 & 0 & 0 \\ \hline
			-1 & -2 & -1 \\ \hline
		\end{tabular}
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\centering
		\begin{tabular}{|l|l|l|} \hline
			-1 & 0 & 1 \\ \hline
			-2 & 0 & 2 \\ \hline
			-1 & 0 & 1 \\ \hline
		\end{tabular}
	\end{minipage} 
\end{table}

\textbf{Laplacian of Gaussian Operator:}For use this method, you need to calculate the derivative of the second order and create a mask on the Laplacian distribution, which is listed below:
\begin{table}[!htb]
	\caption{\textbf{Laplacian of Gaussian Operator}}
	\centering
	\vspace{0.3cm}
	\begin{tabular}{|l|l|l|} \hline
		0 & -1 & 0 \\ \hline
		-1 & 4 & -1 \\ \hline
		0 & -1 & 0 \\ \hline
	\end{tabular} 
\end{table}

\noindent The edge segmentation method has other methods, but all methods mainly work with masks that go through the entire image and converts it to the desired shape.[1210ijcsit14]


\subsection{Histogram-based methods}\label{sec:3.4.4}
\par  Since the other segmentation methods require a full pass through the image, there is a method that works with histograms, and its speed is much faster than the rest. It involves the least time pass on the pixels of the entire image.This process involves grouping the space of an entire dimension, in which all similar objects behave the same way and are grouped into sets, in other words building their histogram. Segmentation in this process is carried out by comparing all the existing groups in the set in its area inside the image, in which all the related parts will look like a group. The intervals between histograms represent a change as hills, where colors shimmer from each other.  The peculiarity here is the gray color that will be distributed between the multi-modal histograms, and the regions will be defined by peaks. Then again you need to define a threshold that will determine what region we want. This motivates the need
threshold value methods, focused on understanding and creating conclusions, in which the selected thresholds are closely related to the histogram and the quality of the region or area.[ch10]


\subsection{Compression-based methods}\label{sec:3.4.5}
\par The compression process is a more advanced segmentation method, so it includes 2 main processes, it is segmenting the image, and further compressing it. A result is a huge number of regions that do not intersect each other, and their combination can lead to a return to the original position, which in some cases is convenient. After the segmentation has occurred, the compression process continues, which compresses all created segments to a specific state or size. Segmentation on the output gives different objects, such as smoothed parts of the image, text, individual images, graphics, and areas that overlap the rest. Smooth areas use a compression method that uses an arithmetic encoder that operates on the basis of contrast and color palettes. The text uses a different method that uses text encoding. Classic JPEG-based compression methods are used for the image.[compressionBased]

\par The segmentation algorithm works together with intensity values. In intensity, it is important to keep in mind the similarity between pixels. Due to the fact that the object is taken as a whole, its edges can be saved intact, to continue to use. The image quality is also maintained. At the stage of compression, for each segment, its type is determined, and then the method by which it will be compressed is determined. For efficiency, segments are encoded into a specific format.
Effective and reliable (adaptive) methods of image compression using remote sensing become more and more necessary both in quantity and in size images for archiving and transferring the purpose of the network is constantly growing. In addition, all remote sensing images have a huge amount frequency component. Therefore, it is important to reach a maximum the degree of compression while maintaining a reasonable the computational complexity of implementation and high the visual quality of the restored image and, possibly, only Segmentation based on image compression.Amount of segmentation compression methods of general and remote control are offered picture.An algorithm for lossless image compression is proposed, using the Segmentation of the size of the variable block. Duplication in the representation of a digital image can be divided into two categories: local and global. In this work, a lossless image compression scheme is used, using redundancy at the local and global levels ensures maximum compression efficiency. This algorithm segments the image into blocks of variable size and encodes them depending on the characteristics presented pixels inside the block. The execution of this algorithm is superior to other lossless compression schemes, such as Huffman, arithmetic, Lempel-Ziv, and JPEG. But the characteristics of estimating the distribution of the image and the resulting efficiency of compression are a very difficult task because of the huge amount of computation.[fulltext24632013]


\subsection{Dual clustering method}\label{sec:3.4.6}
\par This method represents a combination of other methods we are familiar with. It uses the three most important features of image segmentation. The image is taken, segmented using a histogram-based method, then their density and completeness are checked by clustering, and last but not least the boundaries of these segments are checked by the integrity of their gradients. The method is carried out by creating two spaces we need. The first is responsible for the intensity and brightness of the image. Next, in another space is the original image itself, which has a dual feature. As mentioned earlier, the first space checks the intensity and brightness distribution. Everything happens in the clustering process.[Vadim V. Maximov, Alex Pashintsev Gestalt and Image Understanding. GESTALT THEORY 2012, Vol. 34, No.2, 143-166.]


\subsection{Region-growing methods}\label{sec:3.4.6}
\par The last in our list and also one of the most common segmentation methods is the method of regional growth. This method is based on searching for similar and consistent areas within the image. Also, during the search process, similar pixels are merged into a region, which includes similar areas of the image. The method is simple in that it simply merges all similar pixels together to form an area as a whole region.Due to the fact that this method shares a well-connected region, a region well is a clear boundary that provides an excellent segmentation. When the segmentation process is started, there is a search for and an increase in the criterion, which depends on several factors. Segmentation is due to separation and the creation of clear boundaries. Again, the disadvantage is too expensive a number of calculations during the process. Noise and unevenness can also affect the result of this method.[1707.02051]

\par Methods based on the region rely on the assumption that all neighboring pixels in the same region have a similar value or a certain range. This leads to a class of algorithms known as an area whose growth the technique of "splitting and merging", perhaps the most famous. The general procedure is to compare specific one-pixel function for its neighbors. If the homogeneity criterion is satisfied, then the pixel assigned to the same class as one or more of its neighbors. Choice of the criterion of homogeneity critical for even moderate success, and in all cases the results are disappointed by the noise. The fourth type hybrid methods, which combine boundary and regional criteria. This class includes morphological Segmentation of the catchment area and a landing surface of variable order. The catchment method is usually applied to gradient image. This gradient image can be viewed as a topography with boundaries between regions ridges of hills. Segmentation is equivalent to filling the topography of the starting points of the boundaries of the region so that they are open to keeping water from different points of sowing from the meeting. Like the whole perverted method, the technique encounters difficulties with images in which the areas are both noisy, and blurred or fuzzy volume. In addition, this method is also very expensive to calculate. In this article, A segmentation method based on the method of growing the sown area is proposed, which is less sensitive to noisy image and quantitatively and avoid an explosion, leakage, segmentation, and segmentation problems.[1412.3958]



\section{Supervised Learning}\label{sec:3.5}
\par The essence of supervised learning is to find algorithms that build hypotheses based on previously solved problems and you already know the range of conclusions that will be on the output. This method helps to solve the problem for the future and make a forecast for future data. In other words, the goal is to create a concise model that will be trained on these features and their labels. However, the model will be used to predict the data, where there are only signs, but there are no markings. But we know the exact range of these labels.Algorithms for supervised learning can be as classifiers, as regressors.The essence of supervised learning is to find algorithms that build hypotheses based on previously solved problems and you already know the range of conclusions that will be on the output. This method helps to solve the problem for the future and make a forecast for future data. In other words, the goal is to create a concise model that will be trained on these features and their labels. However, the model will be used to predict the data, where there are only signs, but there are no markings. But we know the exact range of these labels.Algorithms for supervised learning can be as classifiers, as regressors.Since the training is inductive, because this algorithm is trained from a certain set of rules, which are usually presented as datasets, or certain parameters are important in the conclusion of the forecast. In short, the task of this tutorial is to recreate an algorithm that is ready for new data instances and works correctly.An important factor in creating an entire training processor supervised learning, data selection and feature selection are paramount, as data can play a cruel joke with you. The process of working with data we can imagine as its pre-processing and transformation. Since only digits that do not contain noise, unnecessary data or just emptiness are needed to work with these algorithms. Removing this kind of data is an important aspect of training.The data can be generated using different machine learning methods. It can also be noted that in most cases a well-assembled dataset is more important than the algorithm used. It can also be too a huge number of signs, as it makes the learning process more difficult and time-consuming. For this reason, dimensionality reduction methods with minimal data loss in the dataset are also an important part of supervised learning.Typically, the feature selection process can be represented as the removal of unnecessary and redundant information, since the datasets are usually sparse. After all, with the usual data collecting data is collected regardless of their measure of need. Also, when we find dependencies, we can remove not strongly correlated features, or we can generate them on the basis of dependent features. This reduces redundancy and also contributes to good algorithm performance.The choice of the algorithm should be approached very carefully and subtly because fully convinced of the data, we can rely only on the algorithm itself.The simplest method of checking algorithms is its accuracy. But we also must not forget about other parameters like recall and precision. In the evaluation of the algorithm, we should consider all the indicators and settle on its previously set task.The simplest method of checking algorithms is its accuracy. But we also must not forget about other parameters like recall and precision. In the evaluation of the algorithm, we should consider all the indicators and settle on its previously set task. Since classification occupies an important part in the work of intelligent systems, its development and speed is an important prerogative. More advanced algorithms supervised learning are neural networks, Bayesian networks, etc. \textbf{Here will be one image about all parts of supervised learning} [sup3]


\par Passable or hidden layers that do not include inputs and outputs are very effective in training, as they add a more complex shape, and the training of convex and non-convex shapes is carried out more easily and possible. Nonlinearities, usually not solvable, can be differentiated and brought into a more convenient form. As well as the relationships between all layers are a good work out this algorithm. Due to the rapid development of neural networks, complex and nonlinear problems in the problems began to be solved with the help of deep learning algorithms. The methods of neural networks are unique in that their error on any instance returns to the very beginning of the perceptron diffusing and changing the weight in the direction we need to improve the result.[sup2]


\section{Optimization}\label{sec:3.6}
\par In computing the function, and composes the important role played by its minimization or else calling optimization. This process is accounted for by minimization functions that reduce the error value in the main target function. It's just a mathematical function that depends on the internal parameters of the model used. Typically, you can retrieve these parameters when you create a dependency on the $Y(X)$ target value calculation from a set of parameters $X$ that will be used inside the prediction method. In a neural network, there are such concepts of weight $W(X)$ and their bias $(B)$, the role of which is to do calculations inside the neural network with input data, converting, minimizing and pulling them on the output layers. Since the neural network problem to learn to predict better, each iteration calculates what proportion of the error of the estimated answers from the present. And the task at this stage is to learn how to minimize this error. This process of minimization plays an extremely important role in the training of neural networks.Passable or hidden layers that do not include inputs and outputs are very effective in training, as they add a more complex shape, and the training of convex and non-convex shapes is carried out more easily and possible. Nonlinearities, usually not solvable, can be differentiated and brought into a more convenient form. As well as the relationships between all layers are a good work out this algorithm. Due to the rapid development of neural networks, complex and nonlinear problems in the problems began to be solved with the help of deep learning algorithms. The methods of neural networks are unique in that their error on any instance returns to the very beginning of the perceptron diffusing and changing the weight in the direction we need to improve the result.The preparation of the model should be approached very carefully, and take into account the parameters of the model, which effectively affects its construction as a regressors or classifier. The quality of the result and its forecast directly depend on the parameters and their value. To select the same parameters, you need to resort to a variety of versatile methods that will update and make the calculation of the corresponding optimization parameters. Model output is indirectly and directly related to the operation of selecting and searching the most optimal model parameters.
Optimization methods can be divided into two main categories, according to which the same optimization algorithms operate. To the first category of optimization methods, we deduce \textit{ first-order optimization algorithms}. These are optimization algorithms that use the first-order differential to minimize the loss function $E (x)$. When this loss function is optimized, a gradient differential is used. The first-order derivative, which is calculated from the calculations of the main function, shows how much the function has changed at a certain point. At the same time, the function can decrease and increase at this point. When visualizing this function, we may notice a tangent to the point where the underlying surface loss is calculated.Here emerges the question, what still is this gradient value. If we look at a gradient in a simpler sense, it's just a vector. A vector that includes all possible variations of the generalization of the derivative $(dy/dx)$. In short, the gradient is how much the specific value of $Y(x)$ from the x parameter changes.the Gradient value is calculated by a purely hourly derivative. Also by calculating the gradient that its output function creates a vector field.The calculated gradient value is represented by a Jacobin matrix that shows all partial derivatives.The difference between the second category of optimization methods, which simply uses a second-order derivative. Also a commonly used name for this species Hessian. This category is very similar, it simply creates a matrix as a vector space that is filled with second-order partial derivatives. The derived function shows not the change of the function, but the change previously calculated by the derivative of the first order. In this case, it is calculated how much the function has warped. And also when visualizing, we can see that it is not a line, more parabola, quadratic surface that touches the line of our loss function [Anish Singh Walia, Types of Optim methods.]


\par One of the most common and widely used optimization methods for neural networks is the gradient descent method. But on the other hand, the implementation of this method in libraries for in-depth training is different in some versions and functions, which causes some oddity among researchers.As mentioned earlier, gradient descent is used to minimize our E(X) function, which will be parameterized by a model that uses parameters from X in the form of O. Minimization is based on the aspiration of the same parameters to the anti-gradient.Anti-gradient is the opposite direction from the main gradient. Speed or as stated differently, the steps are determined by the rate of learning, which shows how to move our value in the direction of anti-gradient. We move along the slope until we reach the local minimum, or sometimes this local minimum is a global minimum.We can separate the gradient descent method by the amount of data that is used to execute the target function. Since the quality and speed of gradient descent change with different amounts of data, it was advisable to divide it into 3 types.[1609.04747][1801.06159][1801.03137][1801.09136][1803.02922]


\par \textbf{Batch gradient descent}. Batch gradient descent works with the entire dataset, and it is calculated during each iteration and with each sample data. It is the most classical method, but it is very expensive in terms of time. The parameters of the model $\theta$ change every era with this formula:


\begin{equation}
\theta = \theta - \eta\nabla_{\theta}E(\theta)
\end{equation}

\par The difficulty in this process presents us with a dataset that may not fit in our RAM, and the problem in calculating the gradient will not be solved. Also the problem is that models can't learn on the fly with new data. After all, the model works so that going through the entire dataset, the process begins back, and cost more in time and capacity. The process includes a priority calculation errors on all of us selected the sample, then computed the vector turns into a matrix with differential of the first order. Next, the calculated matrix uses a Loya update of the main parameters of the neural network or model to approach the minimum point of our target function. The advantage of this method is to calculate the global minimum and the local minimum completely and accurately. The first is computed for convex surfaces, and the second for non-convex surfaces, which is sometimes valuable for researchers.[1609.04747][1801.03137][1801.09136][1803.02922]

\par \textbf{Stochastic gradient descent}. This method, unlike the first one, calculates the gradient and updates the parameters for each package on which the model is trained. Our calculation is made between $x_i$ to each of its target $y(x_i)$:

\begin{equation}
\theta = \theta- {\eta}{\nabla_\theta}E(\theta;x_i;y(x_i))
\end{equation}

\par Because of the batch gradient descent work, which leads to redundancy, the time it takes is expensive, but SGD solves this problem because its gradient computation converges to only one computation in one era. And the instance that is selected for calculation is taken randomly from the dataset. And every time this process repeats itself. Due to the random selection of instances for training, the loss function is very strong and often fluctuates during training, which leads simultaneously to the choice of time for training and to the choice of initial parameters. The difference also lies in the fact that due to a random transition, emissions between local minima are very frequent. There is also a very important role is played by the learning rate, which allows you to navigate between the local minima. After all, when choosing a small learning rate, the transition can be very slow and on the contrary, will worsen the algorithm. And if you select too large, you can jump from the lows and again worsen the algorithm.[1609.04747][1801.03137][1801.09136][1803.02922]



\par \textbf{Mini-batch gradient descent.} This method is special in that it performs a gradient descent among a randomly selected sample, but also selects a certain range in the same sample. This improves the algorithm performance in terms of speed and performance, making it clear that the method works really well.[1801.03137][1801.09136][1803.02922] The formula is written as follows:


\begin{equation}
\theta = \theta- {\eta}{\nabla_\theta}E(\theta;x^{i:i+n};y^{x_i:x_n}) 
\end{equation}

\par \textbf{Momentum}. Despite its performance, the storage system has a number of disadvantages, which also strongly affect the result and lead to slow growth, which further leaves the trained model in local lows. These transitions between lows and highs can be improved by adding a new value that will affect the gradient transition speed. A value, momentum, was taken to smooth the oscillations and control the speed. The effect of momentum we can notice below:

\begin{equation}
\begin{split}
\upsilon_t = \gamma\upsilon_{t-1} + \eta\nabla_\theta E(\theta) \\
\theta = \theta - \upsilon_t
\end{split}
\end{equation}


\par The momentum assigned like $\gamma$, and value mostly is 0.9.Speaking in other words, our momentum is a pusher gradient descent.[1609.04747]


\par \textbf{Nesterov Accelerated Gradient}. This method is explained by the fact that it also allows us to increase the intensity of the change in our gradient descent rate. This is the calculation of the approximate next position of the parameter, which is of course approximated, but without completely updating the parameter, significantly increasing the speed of the algorithm. Since the function changes in the inclined, and allows you to increase the speed, as well as updating each individual parameter of the model[1609.04747].The formula is shown here:

\begin{equation}
\begin{split}
\upsilon_t = \gamma\upsilon_{t-1} + \eta\nabla_\theta E(\theta - \gamma\upsilon_{t-1}) \\
\theta = \theta - \upsilon_t
\end{split}
\end{equation}

\par \textbf{Adagrad}. To improve methods that are based on gradient descent, we can adapt them to certain conditions. The first representative of such methods is Adagrad, which adapts its training by performing certain operations in the process. Under these operations, we mean the moments when the surround or a minor update of the gradient corresponding to its significance. Since not all data is perfect, it is usually very sparse and scattered. It has significantly increased the performance of stochastic gradient descent. While using the Adagrad method, we will not update all the parameters of Our o parameter matrix, and for each of them, we will find a separate learning rate, which will be changed at a certain time step t. First of all, we have to calculate the gradient of the target function in a certain period of time:

\begin{equation}
g_{t,i} = \nabla_{\theta_t} E(\theta_{t,i})
\end{equation}

\noindent SGD process update every parameter in our training parameters $\theta_i$ at each time step $t$, then our parameters updated wholly:

\begin{equation}
\theta_{t+1,i} = \theta_{t,i} - \eta \times g_{t,i}
\end{equation}

\noindent But Adagrad do something other. This process modify main learning rate $\eta$ for each step at $t$ time, here all parameters inside $\theta$ changed, based on the last calculated parameters:

\begin{equation}
\theta_{t+1, i} = \theta_{t,i} - \dfrac{\eta}{\sqrt{G_{t,ii}+\epsilon}}\times g_{t,i}
\end{equation}

\par So a visible advantage Adagrad's is automatically updated and customizable learning rate.But despite all the above advantages, there is a very big drawback of this method, these are positive gradient numbers, which in turn cause the main amount to grow, which causes a slowdown in the speed of learning and obtaining new data, which leads to poor learning with a large number of training.[1609.04747][1801.03137][1801.09136][1803.02922]

\par The perfect form of Adagrad is Adadelta, which, instead of accumulating a huge number of gradients, cuts them off and gives a fixed number of exactly the same gradients. This size $W$ is fixed and controlled.Instead of storing all the data that came out of the gradients, it is more efficient to use its mean $E[g^2]_t$ for all decreasing gradient squares, which gives only the mean and the gradient that was calculated at that point:


\begin{equation}
E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t
\end{equation}

\par Next, using momentum, we update the parameters using the update vector $\Delta\Theta_t$, which will be calculated by the gradient $g_{t,i}$ and learning rate $\eta$:

\begin{equation}
\begin{split}
\Delta\theta_t = - \eta g_{t,i} \\
\theta_{t+1} = \theta_t + \Delta\theta_t
\end{split}
\end{equation}

\par Next one is updating parameters of Adagrad:

\begin{equation}
\Delta\theta_t = - \dfrac{\eta}{\sqrt{G_t+ \epsilon} } \odot g_t
\end{equation}


\par So, for improving results, we replace the gradient matrix with our mean matrix. And we can that our formula converts to other RMS function:

\begin{equation}
\begin{split}
\Delta\theta_t = - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t \\
\Delta\theta_t = - \dfrac{\eta}{RMS[g]_t} g_t
\end{split}
\end{equation}

\par Last thing about Adadelta, is that you don't need initial and static learning rate, because this algorithm do it for its own.[1609.04747][1801.03137][1801.09136][1803.02922]

\section{Backpropagation}\label{sec:3.7}
\par A multilayer neural network for training using the method of inverse spread, which in training practicing on perceptron neural network. The classical neural network is a compositional one, of which there are 3 layers. The input layer, hidden layer, and output layer are these 3 layers. In layers are neurons that build the Foundation for the work of the neural network. The importance, connection, and involvement of each neuron represent it, which in the training will change in a different range. At the very first iteration, weights are determined by random weights. After the values from the input neurons reach the most recent output layer, a loss function is calculated that shows how close or far you are away from the real value. After that, the loss function returns to the previously hidden layer, which updates its weight, after all, the process repeats many times to reach the minimum value of the loss function, as well as to improve the result of prediction and guessing.To paint the back-propagation algorithm can be like this: First of all, you need to understand that the reverse propagation algorithm works in training and test mode. Since the algorithm is a representative of supervised learning algorithms, we already know labels during the learning process. Since the last layer allows you to calculate the loss functions, this data is returned back to the input layer, and then change the weight to one or the other side to achieve a certain result.First of all, you need to understand that the algorithm of reverse distribution works in training and test mode. Since the algorithm is a representative of supervised learning algorithms, we already know labels during the learning process. Since the last layer allows you to calculate the loss functions, this data is returned back to the input layer, and then change the weight to one or the other side to achieve a certain result. This is followed by the choice of the style in which the reverse distribution method will work. Before you start working with this method, you should choose a style of work, which is divided into an approach in which after each return value from the output layer immediately follows the update weights, as well as an approach in which under a certain condition and sequence to update all weights. The second approach wins in time because it does not require mathematical operations to update all weights.Also, in the reverse distribution process, you should understand that all hidden layers are input layers for the following layers, which allows them to be updated in their scales. The last layer, after it has received values last layers immediately calculates the value of the activation function, which is determined in advance.[back2]

\vspace{-0.5cm}
\noindent This algorithm can stop only if the researcher considered that the loss function is small enough and the accuracy reached the desired measure. First of all, the data should go through all the layers, calculating the loss function at the end. Next, after that, you need to perform the reverse distribution operation for the output layer and for the hidden layer, then the output data updates the weights of all previous layers.[NeuralNetworks]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Neural Networks}\label{sec:3.8}
\vspace{-0.5cm}
\noindent One of the most powerful tools of today's intelligent machines is the neural network, which in the last few years has only gained a more powerful turnover in development. Since the penetration of neural networks is high, the area of their use is wide-range, as it can be used for regression, classification, and clustering. Initially, the neural network was invented to try to mathematically reveal the brain and perform its most simple and primitive functions. But with the development of science, of the simple functions, it could perform even the most difficult. The initial task was not only to imitate the brain but also the ability to learn from examples. Under the best conditions with the data where they will snatch all cases, the neural network is capable of showing new trends and extensions in the data, which is very useful for intelligent machines.The simplest example of a neural network is a perceptron, which has 3 main layers that we passed in advance. The main working and important component are the weights between the layers, which are influenced by the training of the neural network.
Neural networks by type of training can be divided into two parts, this is when we know the answer interval, and when we do not know what will be the output. In short, regression/classification and clustering. Important components in the training of the neural network are the activation function, momentum, and pruning.Main object in neural network is neuron.\textbf{The Neuron} is a link that performs all the basic functions in the training of the neural network, using all the nodes in which there are weights for these neurons themselves. Here we can explain so that the stronger and stronger the relationship between neurons, the faster and more efficiently transmit data. This process is very similar to the work of the human brain.The architecture of a simple neural network can be explained very simply. It can be divided into three main layers. This is the input layer, followed by hidden layers, which can be a large number. The most recent layer, the output layer, is used to compute the activation function overall data that has passed the entire neural network. When learning back propagation, this loss function is used to update all weights.You can control the neural network by updating and installing new weights that show the importance of each neuron. Because of the unknown during training, weights at the beginning are always put out of random numbers. The learning process of neural network consists of straight and bra spread. First and foremost are calculated amounts were neurons, as well goes along the bias. The Apostle goes on with the present result. After that, the error is calculated, the error goes back, then the update was, and the process repeats depending on the need and the number of an era. Next, VA performs the function activation role, which is performed by the who so. The peculiarity of this process is that it can solve problems. That and makes NN very Mo weapons. To activate the function, we can use linear, threshold, polynomial and sigmoid functions. Also, the speed and power of the neural network can increase from the removal of unnecessary neurons and connections between them. This is called pruning.[Neural Networks]

\subsection{Convolutional Neural Networks}\label{sec:3.8.1}
\par Convolutional neural networks are a very powerful and productive tool for solving problems in deep and machine learning. Their mathematical content is a very important and practical is justified.Since convolution neural networks are primarily algorithms of supervised learning, you need to write a basic function that will build a hypothesis. To construct a function of the conjecture that will reduce the error function, we will consider in one regression classification formula. This we can do by designating our label one-dimensional size.We know that the prediction algorithm is based on a simple formula $ Y = f(X)$ , in which the function $f$ will always be different depending on the approach and the type of solution. Our task is always to minimize the loss function $E(X)$, which will be the loss $L$ itself, showing how far the predicted value has gone from the real value.Our problem is how to minimize the error that pulls out our function $f$ consisting of a group of all possible conjectures $F$:


\begin{equation}
\hat{f} = \underset{f\in F}{arg\min}E[L(Y,f(X))]
\end{equation}

\par A function to represent all parameters in the model and data in cases that with the classifier that with the practical regression are the same. This representation of the parameters is carried out by changing and transforming all the parameters from the sample, and then this transformation occurs in the prediction function itself.[1605.09081]  For the classifier and for the regressors, we can present this:

\begin{equation}
\begin{split}
f(X) = \langle \Phi(X), w\rangle - regression \\
f(X) = sign(\langle \Phi(X), w\rangle) -  classification
\end{split}
\end{equation}

\par The most powerful and accurate tool for recognition and to determine what is and what is happening in the image are actively used neural networks. They are specifically configured and made to work with images.For this reason, CNN forms its layer of three important values: width, height, and depth. The latter is used to work with color images.\textbf{Convolutional Layer}: These layers are used to address specific issues that MLPs often encounter or fully-connected NN. This solution combats redundancy and noise that classical computer vision techniques could not solve. The difference from other layers is that each neuron in a new layer, it's put together subsets of the previous layer. In short, we compress and weld the squares from the previous neuron, which solves the problem with a huge amount of data, and at the same time does it without losing all the data. The square is sprayed with height and width. Can be called differently as a receptive field that represents a subset of the pixels of the previous layer. The depth is also determined at the very beginning, as we decide to work only with black and white or RGB image. There is also a hyperparameter that corresponds to the number of convolutions we get at the output, it is called - stride. He controls the expense of gap between squares subsets. Mathematical it is clear that there will be fewer neurons at the output than at the input to regulate this process uses a method in which 0 is added to the boundaries. It's called padding. The number of neurons in the output we can calculate by taking away the number of neurons in the input number of receptive field, then adding padding to it, then divide it into stride, then adding to it only the number 1. \textbf{Pooling layer:}the layer which serves to unite as convolutional layers only has the distinctive feature which helps to solve the main problems of NN. The process is similar to convolution, but the difference is that this layer is not parameterized. In this layer there is also no weights or rejection; instead, it simply combines all the neurons in a specific area. The most used method is to take the maximum number of the selected area, as well as methods in which to take the average number. Process on the idea of similar. [1803.02129] 


\subsection{Recurrent Neural Networks}\label{sec:3.8.2}
\par  AN that have a link to a recurrent function are called recurrent neural networks that are able to mathematically work with data that have a particular link and have a specific sequence. The essence of this neural network is that hidden layers save information from the past iteration, and work as memory inside the neural network. This structure allows RNNs to store, remember and process complex signals over long periods of time. RNNs can map the input sequence to the output sequence in the current time step and predict the sequence in the next time step.RNN, this as mentioned earlier, is a class of algorithms superior learning, in which a neural loop is connected, which repeats a certain order of time, and which has memory to work with a sequence. RNN is trained with a dataset where there is the concept of "input - purpose". Optimization comes at the expense of minimizing the difference between the real value and the forecast, but the work is in pairs, as well as these values are stored, and the size of the RNN can be significantly small from CNN.
The neural network has 3 layers: the input layer, which contains all the original neurons. Next are the recurrent hidden layers, which store the past weight and changes. After all, this is the output layer. The entrance is $n$ number of units. These data units look like a sequence of vectors at a certain time $t$ as ${...,x_{t-1},x_t,x_{t+1}}$, where each $x_t$ is from $t$, $x_t = (x_1, x_2, ...,x_N)$ a group of all $X$ units. Next, fully connected input signals are on hidden layers, where their relationship is defined by a matrix of weights $W_{IH}$. Next, the hidden layers create their $M$ hidden units $h_t = (h_1, h_2,...,h_M)$, which are all of them connected recursively. Hidden layers we can define as true neural network memory.Memory $h_t$ in hidden layers is determined by the function's deduction formula from $o_t$, which in turn represents the linear sum of all layers:[1801.01078]

\begin{equation}
h_t = f_H(o_t)
\end{equation}

\begin{equation}
o_t = W_{IH}x_t + W_{HH}h_{t-1} + b_h
\end{equation}

\par Further hidden layers create a connection from the output layer through the weights in the $W_{HO}$. And the output layer has its own $P$ number of units $y_t = (y_1, y_2, ..., y_P)$that are calculated by this formula:

\begin{equation}
y_t = f_O(W_{HO}h_t + b_o)
\end{equation}

\par Where $f_O$ is the activation function for solving nonlinearity and control numbers in RNN. RNN, in its simplicity, consists of nonlinear equations that simply repeat in a certain order of time. Then, in each interval, the loss function is predicted and deducted. Hidden layers perform a memory function that retains the weights and the rest of the desired and best parameters that were calculated in time series. The peculiarity here is that you can determine the future behavior of the network, and significantly improve the result. The simplicity of RN is also caused by the use of simple nonlinear functions, which create dynamics and good training on data in a certain interval.[1801.01078][1710.03414]

\subsection{Capsules Neural Networks}\label{sec:3.8.3}
\par Capsule neural networks have been a new approach to neural networks since October 17. Capsules in NN represent a group of neurons, where their vector, which is responsible for the activity, as well as this vector itself,  represents the parameters of each instance of a particular type of entity, or in other words the pattern. This approach uses the length of the same activity vector to calculate and represent the probability that an instance pattern exists and to represent its parameters. Capsules that are active, calculate specific forecast parameters of an implementation of the capsules, whose levels are more higher. When this prediction occurs, and most of these predictions converge to the same result, the high-level capsule moves to the active capsule level. Usually, after convolutional networks used activation function, which outputs the data to enter in the capsule. The very first capsules serve by simple logic, as multidimensional transformable objects, which are very similar to reverse rendering. This process has not been applied before, which makes it very special.[1710.09829]




\section{Summary}\label{sec:3.9}
\par In this Chapter, we fully explained all the existing algorithms so briefly and clearly, but at the same time openly explained their algorithms and processes with maximum clarity. In the next Chapter, some methods which will be included in the basic work of the algorithm will be revealed. The following will show what kinds of algorithms we use to demonstrate the performance of SS and CNN.