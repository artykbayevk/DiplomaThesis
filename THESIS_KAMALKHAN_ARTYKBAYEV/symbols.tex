\newpage
\newenvironment{symbols}{%
	\clearpage
	\chapter*{\listsymbolname}
	\beginsymlist}
{\closesymlist}
%
\newcommand{\beginsymlist}{%
	\begin{tabbing}
		\hspace*{\symtabi}\=\hspace*{\symtabii}\=\kill}
	%
	\newcommand{\closesymlist}{\end{tabbing}}
%
\newcommand{\sym}[2]{\>#1 \>#2 \\}
\newlength{\symtabi}
\newlength{\symtabii}
\setlength{\symtabi}{1em}
\setlength{\symtabii}{5em}
\newcommand{\listsymbolname}{LIST OF SYMBOLS/ABBREVIATIONS}/

\begin{symbols}
	\sym{$ MSE $}{Mean-square-error}
	\sym{$CNN$ }{Convolutional neural network}
	\sym{$RNN $}{Recurrent Neural Network}
	\sym{$CV$ }{Computer Vision}
	\sym{$ML$ }{Machine Learning}
	\sym{$MLP$ }{Multilayer Perceptron}
	\sym{$LRT$ }{Learning Rate}
	\sym{$Conv$ }{Convolutional layer}
	\sym{$Pool$ }{Pooling layer}
	\sym{$ReLU$ }{Rectified Linear Unit}
	\sym{$Softmax$ }{Normalized Exponential Function}
	\sym{$Sigm$ }{Special case of logistic function}
	\sym{$SS$ }{Selective Search}
	\sym{$\kappa$}{Clusters count}
	\sym{$C_{\kappa}$}{Cluster}
	\sym{$m_{\kappa}$}{Mean of cluster}
	\sym{$D$ or $E\left( x \right) $}{Function of error}
	\sym{$L$}{Loss}
	\sym{$\theta$}{Parameter of training}
	\sym{$\gamma$}{Monentum parameter}
	\sym{$g$}{Gradient}
	\sym{$G$}{Gradient matrix i$\times$i}
	\sym{$\epsilon$}{Smoothing coefficient}
	\sym{$\eta$}{Learning rate}
	% \sym{$\delta$}{Regularization parameter} \sym{$\varepsilon$}{Reweighting parameter} \sym{$\gamma$}{Control parameter} \sym{$\kappa$}{Adjusting parameter} \sym{$\lambda$}{Eigenvalue} \sym{$\mu$}{Step-size} \sym{$\wp$}{Correlation parameter} \sym{$\sigma ^2$}{Variance} \sym{$\textbf{I}$}{Identity matrix} \sym{$\textbf{Q}$}{control matrix} \sym{$N$}{Filter length}  \sym{$\textbf{R}$}{Autocorrelation matrix} \sym{$\textbf{w}$}{Tap weights vector} \sym{$\textbf{x}$}{Tap input vector} \sym{AEC}{Acoustic echo canceller} \sym{AWGN}{Additive white Gaussian noise} \sym{ACGN}{Additive correlated Gaussian noise}\sym{CS}{Compressive sensing} \sym{DSP}{Digital signal processing}  \sym{FIR}{Finite impulse response} \sym{IPNLMS}{Improved proportionate normalized least-mean-square} \sym{LMS}{Least-mean-square} \sym{MSE}{Mean-square-error}  \sym{MSD}{Mean square deviation} \sym{NLMS}{Normalized least-mean-square} \sym{NNCLMS}{Non-uniform norm constraint LMS} \sym{PNLMS}{Proportionate normalized least-mean-square} \sym{RZA-LMS}{Reweighted zero-attracting LMS}  \sym{SNR}{Signal-to-noise ratio} \sym{VSSLMS}{Variable step size LMS} \sym{ZA-LMS}{Zero-attracting LMS}
\end{symbols}
