{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Code\n",
    "## Prepare data from directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all need libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from skimage import io, transform\n",
    "from random import randint\n",
    "\n",
    "import os, uuid, glob, warnings\n",
    "import numpy as np\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathes for images and their annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = '../data/fl27/original'\n",
    "CROPPED_DATA_DIR = '../data/fl27/images'\n",
    "ORIGINAL_ANNOTATION = '../data/fl27/annotation.txt'\n",
    "CROPPED_ANNOTATION = '../data/fl27/crop_annotation.txt'\n",
    "\n",
    "TRAIN_SET = '../annotations/trainset.txt'\n",
    "TEST_SET = '../annotations/testset.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original images of logos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_files = glob.glob(os.path.join(ORIGINAL_DATA_DIR, '*.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_annotation(path):\n",
    "    file = open(path, \"r\")\n",
    "    content = file.readlines()\n",
    "    new = [x.split(\" \")[:-1] for x in content]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing labels list from annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(annotaton_path):\n",
    "    arr = read_from_annotation(annotaton_path)\n",
    "    labels = []\n",
    "    for item in (arr):\n",
    "        l = item[1]\n",
    "        if l not in labels:\n",
    "            labels.append(l)\n",
    "        else:\n",
    "            continue\n",
    "    return labels\n",
    "# LABELS = prepare_labels(ORIGINAL_ANNOTATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating crops and resize them for creating test and train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resized_crops(input_annotation, input_directory,\n",
    "                        output_annotation, output_directory):\n",
    "    annotation = read_from_annotation(input_annotation)\n",
    "    output_annotation = open(output_annotation, 'w')\n",
    "    for i, data in zip(range(len(annotation)),annotation):\n",
    "        img_name = data[0]\n",
    "        img_path = os.path.join(input_directory, img_name)\n",
    "        image = io.imread(img_path)\n",
    "        positions = data[-4:]\n",
    "        x1 = int(positions[0])\n",
    "        y1 = int(positions[1])\n",
    "        x2 = int(positions[2])\n",
    "        y2 = int(positions[3])\n",
    "\n",
    "        if x1 > x2:\n",
    "            tmp = x1\n",
    "            x1 = x2\n",
    "            x2 = tmp\n",
    "\n",
    "        if y1 > y2:\n",
    "            tmp = y1\n",
    "            y1 = y2\n",
    "            y2 = tmp\n",
    "        \n",
    "        \n",
    "            \n",
    "        target = data[1]\n",
    "#         fl32 - dataset\n",
    "        crop_resizer(image, target, x1,x1+x2,y1,y1+y2,output_annotation, output_directory)\n",
    "#         fl27 - dataset\n",
    "        crop_resizer(image, target, x1,x2,y1,y2,output_annotation, output_directory)\n",
    "#         crop_resizer(image, target, x1-40,x2,y1,y2,output_annotation, output_directory)\n",
    "#         crop_resizer(image, target, x1,x2+40,y1,y2,output_annotation, output_directory)\n",
    "#         crop_resizer(image, target, x1,x2,y1-40,y2,output_annotation, output_directory)\n",
    "#         crop_resizer(image, target, x1,x2,y1,y2+40,output_annotation, output_directory)\n",
    "\n",
    "def crop_resizer(image,target, x1,x2,y1,y2, writer,directory):\n",
    "    crop_img = image[y1:y2, x1:x2]\n",
    "    file_name = str(uuid.uuid4().hex)+'.jpg'\n",
    "    try:\n",
    "        try:\n",
    "            resized = transform.resize(crop_img,(224, 224))\n",
    "            io.imsave(os.path.join(directory, file_name), resized)\n",
    "            note = \"{} {} \\n\".format(file_name, target)\n",
    "            writer.write(note)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    except IndexError:\n",
    "        pass\n",
    "#create_resized_crops(ORIGINAL_ANNOTATION, ORIGINAL_DATA_DIR, CROPPED_ANNOTATION, CROPPED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing images into train and test dataset, prepare SET.TXT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(crop_ann_path, out_directory, test_size = 1000):\n",
    "    train = read_from_annotation(crop_ann_path)\n",
    "    test = []\n",
    "    for i in range(test_size):\n",
    "        randIndex = randint(0,len(train)-1)\n",
    "        test.append(train[randIndex])\n",
    "        del(train[randIndex])\n",
    "\n",
    "    trainset = file(TRAIN_SET, \"w\")\n",
    "    testset = file(TEST_SET, \"w\")\n",
    "\n",
    "    for item in train:\n",
    "        tmp = \"{} {} \\n\".format(item[0], item[1])\n",
    "        trainset.write(tmp)\n",
    "    trainset.close()\n",
    "\n",
    "    for item in test:\n",
    "        tmp = \"{} {} \\n\".format(item[0], item[1])\n",
    "        testset.write(tmp)\n",
    "    testset.close()\n",
    "# train_test_split(CROPPED_ANNOTATION,RESIZED_ANNOTATION,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pytorch dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pytorch dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_file, root):\n",
    "        self.txt_file = txt_file\n",
    "        self.root = root\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.txt_file.shape[0]\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        img_name = os.path.join(self.root, self.txt_file[id][0])\n",
    "        img = io.imread(img_name)\n",
    "        logo = int(self.txt_file[id][1])\n",
    "        image = img.transpose((2, 0, 1))\n",
    "        img = torch.FloatTensor(image)\n",
    "        return img,logo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My main labels(in future I want to add new class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adidas', 'Apple', 'BMW', 'Citroen', 'Cocacola', 'DHL', 'Fedex', 'Ferrari', 'Ford', 'Google', 'Heineken', 'HP', 'Intel', 'McDonalds', 'Mini', 'Nbc', 'Nike', 'Pepsi', 'Porsche', 'Puma', 'RedBull', 'Sprite', 'Starbucks', 'Texaco', 'Unicef', 'Vodafone', 'Yahoo']\n"
     ]
    }
   ],
   "source": [
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert function from simple list into numpy array with categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_num_dataset(annotation_path, set_path):\n",
    "    arr = read_from_annotation(set_path)\n",
    "    out = []\n",
    "    for item in arr:\n",
    "        tmp = [item[0], LABELS.index(item[1])]\n",
    "        out.append(tmp)\n",
    "    out = np.array(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating main train and test set, which are suitable for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 16185 items and test size 5000 items\n"
     ]
    }
   ],
   "source": [
    "train_data = prepare_num_dataset(CROPPED_ANNOTATION, TRAIN_SET)\n",
    "test_data = prepare_num_dataset(CROPPED_ANNOTATION, TEST_SET)\n",
    "trainset = MyDataset(train_data, CROPPED_DATA_DIR)\n",
    "testset = MyDataset(test_data, CROPPED_DATA_DIR)\n",
    "print(\"Train size {} items and test size {} items\".format(len(trainset), len(testset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Data Loaders, which send for training batches with fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                           batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Convolutional Neural Network with 27 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without any class balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize hyper parameters of CNN and number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "n_classes = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating CNN class and initialize it with number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, n_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "cnn = CNN(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Iter [10/404] Loss: 3.3259\n",
      "Epoch [1/30], Iter [20/404] Loss: 3.1986\n",
      "Epoch [1/30], Iter [30/404] Loss: 3.3115\n",
      "Epoch [1/30], Iter [40/404] Loss: 3.2043\n",
      "Epoch [1/30], Iter [50/404] Loss: 3.1178\n",
      "Epoch [1/30], Iter [60/404] Loss: 2.9768\n",
      "Epoch [1/30], Iter [70/404] Loss: 3.2297\n",
      "Epoch [1/30], Iter [80/404] Loss: 2.8930\n",
      "Epoch [1/30], Iter [90/404] Loss: 2.8964\n",
      "Epoch [1/30], Iter [100/404] Loss: 3.0892\n",
      "Epoch [1/30], Iter [110/404] Loss: 2.6049\n",
      "Epoch [1/30], Iter [120/404] Loss: 2.7870\n",
      "Epoch [1/30], Iter [130/404] Loss: 2.6302\n",
      "Epoch [1/30], Iter [140/404] Loss: 2.5602\n",
      "Epoch [1/30], Iter [150/404] Loss: 2.5616\n",
      "Epoch [1/30], Iter [160/404] Loss: 2.7008\n",
      "Epoch [1/30], Iter [170/404] Loss: 2.3921\n",
      "Epoch [1/30], Iter [180/404] Loss: 2.5160\n",
      "Epoch [1/30], Iter [190/404] Loss: 2.7436\n",
      "Epoch [1/30], Iter [200/404] Loss: 2.4593\n",
      "Epoch [1/30], Iter [210/404] Loss: 2.6125\n",
      "Epoch [1/30], Iter [220/404] Loss: 2.0055\n",
      "Epoch [1/30], Iter [230/404] Loss: 2.1821\n",
      "Epoch [1/30], Iter [240/404] Loss: 2.0208\n",
      "Epoch [1/30], Iter [250/404] Loss: 2.1114\n",
      "Epoch [1/30], Iter [260/404] Loss: 1.8999\n",
      "Epoch [1/30], Iter [270/404] Loss: 1.9664\n",
      "Epoch [1/30], Iter [280/404] Loss: 2.3798\n",
      "Epoch [1/30], Iter [290/404] Loss: 2.0220\n",
      "Epoch [1/30], Iter [300/404] Loss: 1.8832\n",
      "Epoch [1/30], Iter [310/404] Loss: 2.0820\n",
      "Epoch [1/30], Iter [320/404] Loss: 2.1483\n",
      "Epoch [1/30], Iter [330/404] Loss: 1.6048\n",
      "Epoch [1/30], Iter [340/404] Loss: 1.3389\n",
      "Epoch [1/30], Iter [350/404] Loss: 1.8789\n",
      "Epoch [1/30], Iter [360/404] Loss: 1.9313\n",
      "Epoch [1/30], Iter [370/404] Loss: 1.5931\n",
      "Epoch [1/30], Iter [380/404] Loss: 1.5519\n",
      "Epoch [1/30], Iter [390/404] Loss: 1.6138\n",
      "Epoch [1/30], Iter [400/404] Loss: 1.4405\n",
      "Epoch [2/30], Iter [10/404] Loss: 1.2850\n",
      "Epoch [2/30], Iter [20/404] Loss: 2.2057\n",
      "Epoch [2/30], Iter [30/404] Loss: 1.8828\n",
      "Epoch [2/30], Iter [40/404] Loss: 1.4380\n",
      "Epoch [2/30], Iter [50/404] Loss: 1.5429\n",
      "Epoch [2/30], Iter [60/404] Loss: 1.2234\n",
      "Epoch [2/30], Iter [70/404] Loss: 1.6877\n",
      "Epoch [2/30], Iter [80/404] Loss: 1.7219\n",
      "Epoch [2/30], Iter [90/404] Loss: 1.6200\n",
      "Epoch [2/30], Iter [100/404] Loss: 1.2700\n",
      "Epoch [2/30], Iter [110/404] Loss: 1.0385\n",
      "Epoch [2/30], Iter [120/404] Loss: 1.4550\n",
      "Epoch [2/30], Iter [130/404] Loss: 1.3394\n",
      "Epoch [2/30], Iter [140/404] Loss: 1.1801\n",
      "Epoch [2/30], Iter [150/404] Loss: 0.9672\n",
      "Epoch [2/30], Iter [160/404] Loss: 1.4639\n",
      "Epoch [2/30], Iter [170/404] Loss: 1.7063\n",
      "Epoch [2/30], Iter [180/404] Loss: 1.1354\n",
      "Epoch [2/30], Iter [190/404] Loss: 1.1051\n",
      "Epoch [2/30], Iter [200/404] Loss: 1.0963\n",
      "Epoch [2/30], Iter [210/404] Loss: 1.5825\n",
      "Epoch [2/30], Iter [220/404] Loss: 1.3130\n",
      "Epoch [2/30], Iter [230/404] Loss: 0.8632\n",
      "Epoch [2/30], Iter [240/404] Loss: 1.3354\n",
      "Epoch [2/30], Iter [250/404] Loss: 1.0164\n",
      "Epoch [2/30], Iter [260/404] Loss: 0.7740\n",
      "Epoch [2/30], Iter [270/404] Loss: 1.1009\n",
      "Epoch [2/30], Iter [280/404] Loss: 1.0332\n",
      "Epoch [2/30], Iter [290/404] Loss: 1.1515\n",
      "Epoch [2/30], Iter [300/404] Loss: 0.6704\n",
      "Epoch [2/30], Iter [310/404] Loss: 1.2301\n",
      "Epoch [2/30], Iter [320/404] Loss: 1.0470\n",
      "Epoch [2/30], Iter [330/404] Loss: 0.9997\n",
      "Epoch [2/30], Iter [340/404] Loss: 1.1674\n",
      "Epoch [2/30], Iter [350/404] Loss: 1.3094\n",
      "Epoch [2/30], Iter [360/404] Loss: 0.8985\n",
      "Epoch [2/30], Iter [370/404] Loss: 0.7709\n",
      "Epoch [2/30], Iter [380/404] Loss: 1.5332\n",
      "Epoch [2/30], Iter [390/404] Loss: 0.7560\n",
      "Epoch [2/30], Iter [400/404] Loss: 1.2477\n",
      "Epoch [3/30], Iter [10/404] Loss: 0.8730\n",
      "Epoch [3/30], Iter [20/404] Loss: 1.3409\n",
      "Epoch [3/30], Iter [30/404] Loss: 1.5823\n",
      "Epoch [3/30], Iter [40/404] Loss: 1.0203\n",
      "Epoch [3/30], Iter [50/404] Loss: 0.7372\n",
      "Epoch [3/30], Iter [60/404] Loss: 1.0293\n",
      "Epoch [3/30], Iter [70/404] Loss: 0.6705\n",
      "Epoch [3/30], Iter [80/404] Loss: 0.7080\n",
      "Epoch [3/30], Iter [90/404] Loss: 0.6463\n",
      "Epoch [3/30], Iter [100/404] Loss: 1.0929\n",
      "Epoch [3/30], Iter [110/404] Loss: 0.7875\n",
      "Epoch [3/30], Iter [120/404] Loss: 1.1534\n",
      "Epoch [3/30], Iter [130/404] Loss: 1.0088\n",
      "Epoch [3/30], Iter [140/404] Loss: 1.0355\n",
      "Epoch [3/30], Iter [150/404] Loss: 0.6066\n",
      "Epoch [3/30], Iter [160/404] Loss: 0.6716\n",
      "Epoch [3/30], Iter [170/404] Loss: 1.0801\n",
      "Epoch [3/30], Iter [180/404] Loss: 0.9346\n",
      "Epoch [3/30], Iter [190/404] Loss: 0.7828\n",
      "Epoch [3/30], Iter [200/404] Loss: 0.8711\n",
      "Epoch [3/30], Iter [210/404] Loss: 0.9774\n",
      "Epoch [3/30], Iter [220/404] Loss: 0.5256\n",
      "Epoch [3/30], Iter [230/404] Loss: 0.8071\n",
      "Epoch [3/30], Iter [240/404] Loss: 1.1261\n",
      "Epoch [3/30], Iter [250/404] Loss: 0.3915\n",
      "Epoch [3/30], Iter [260/404] Loss: 0.9307\n",
      "Epoch [3/30], Iter [270/404] Loss: 0.6226\n",
      "Epoch [3/30], Iter [280/404] Loss: 0.8756\n",
      "Epoch [3/30], Iter [290/404] Loss: 0.4201\n",
      "Epoch [3/30], Iter [300/404] Loss: 0.7132\n",
      "Epoch [3/30], Iter [310/404] Loss: 0.6877\n",
      "Epoch [3/30], Iter [320/404] Loss: 0.8511\n",
      "Epoch [3/30], Iter [330/404] Loss: 0.6802\n",
      "Epoch [3/30], Iter [340/404] Loss: 0.5714\n",
      "Epoch [3/30], Iter [350/404] Loss: 0.3688\n",
      "Epoch [3/30], Iter [360/404] Loss: 0.7270\n",
      "Epoch [3/30], Iter [370/404] Loss: 0.4907\n",
      "Epoch [3/30], Iter [380/404] Loss: 0.9384\n",
      "Epoch [3/30], Iter [390/404] Loss: 0.5972\n",
      "Epoch [3/30], Iter [400/404] Loss: 0.5278\n",
      "Epoch [4/30], Iter [10/404] Loss: 0.6405\n",
      "Epoch [4/30], Iter [20/404] Loss: 0.4413\n",
      "Epoch [4/30], Iter [30/404] Loss: 0.4946\n",
      "Epoch [4/30], Iter [40/404] Loss: 0.4414\n",
      "Epoch [4/30], Iter [50/404] Loss: 0.4058\n",
      "Epoch [4/30], Iter [60/404] Loss: 0.3688\n",
      "Epoch [4/30], Iter [70/404] Loss: 0.3345\n",
      "Epoch [4/30], Iter [80/404] Loss: 0.3047\n",
      "Epoch [4/30], Iter [90/404] Loss: 0.7857\n",
      "Epoch [4/30], Iter [100/404] Loss: 0.5279\n",
      "Epoch [4/30], Iter [110/404] Loss: 0.3954\n",
      "Epoch [4/30], Iter [120/404] Loss: 0.7486\n",
      "Epoch [4/30], Iter [130/404] Loss: 0.5732\n",
      "Epoch [4/30], Iter [140/404] Loss: 0.3871\n",
      "Epoch [4/30], Iter [150/404] Loss: 0.5429\n",
      "Epoch [4/30], Iter [160/404] Loss: 0.4885\n",
      "Epoch [4/30], Iter [170/404] Loss: 0.2940\n",
      "Epoch [4/30], Iter [180/404] Loss: 0.3205\n",
      "Epoch [4/30], Iter [190/404] Loss: 0.3705\n",
      "Epoch [4/30], Iter [200/404] Loss: 0.4895\n",
      "Epoch [4/30], Iter [210/404] Loss: 0.4575\n",
      "Epoch [4/30], Iter [220/404] Loss: 0.4875\n",
      "Epoch [4/30], Iter [230/404] Loss: 0.4239\n",
      "Epoch [4/30], Iter [240/404] Loss: 0.4899\n",
      "Epoch [4/30], Iter [250/404] Loss: 0.2958\n",
      "Epoch [4/30], Iter [260/404] Loss: 0.4083\n",
      "Epoch [4/30], Iter [270/404] Loss: 0.3093\n",
      "Epoch [4/30], Iter [280/404] Loss: 0.5567\n",
      "Epoch [4/30], Iter [290/404] Loss: 0.8869\n",
      "Epoch [4/30], Iter [300/404] Loss: 0.3606\n",
      "Epoch [4/30], Iter [310/404] Loss: 0.3183\n",
      "Epoch [4/30], Iter [320/404] Loss: 0.3520\n",
      "Epoch [4/30], Iter [330/404] Loss: 0.3283\n",
      "Epoch [4/30], Iter [340/404] Loss: 0.4245\n",
      "Epoch [4/30], Iter [350/404] Loss: 0.4588\n",
      "Epoch [4/30], Iter [360/404] Loss: 0.5385\n",
      "Epoch [4/30], Iter [370/404] Loss: 0.8355\n",
      "Epoch [4/30], Iter [380/404] Loss: 0.6134\n",
      "Epoch [4/30], Iter [390/404] Loss: 0.3255\n",
      "Epoch [4/30], Iter [400/404] Loss: 0.5320\n",
      "Epoch [5/30], Iter [10/404] Loss: 0.5321\n",
      "Epoch [5/30], Iter [20/404] Loss: 0.3338\n",
      "Epoch [5/30], Iter [30/404] Loss: 0.2159\n",
      "Epoch [5/30], Iter [40/404] Loss: 0.5665\n",
      "Epoch [5/30], Iter [50/404] Loss: 0.4652\n",
      "Epoch [5/30], Iter [60/404] Loss: 0.1542\n",
      "Epoch [5/30], Iter [70/404] Loss: 0.4428\n",
      "Epoch [5/30], Iter [80/404] Loss: 0.4326\n",
      "Epoch [5/30], Iter [90/404] Loss: 0.3704\n",
      "Epoch [5/30], Iter [100/404] Loss: 0.4444\n",
      "Epoch [5/30], Iter [110/404] Loss: 0.3114\n",
      "Epoch [5/30], Iter [120/404] Loss: 0.2735\n",
      "Epoch [5/30], Iter [130/404] Loss: 0.5978\n",
      "Epoch [5/30], Iter [140/404] Loss: 0.2473\n",
      "Epoch [5/30], Iter [150/404] Loss: 0.1702\n",
      "Epoch [5/30], Iter [160/404] Loss: 0.3386\n",
      "Epoch [5/30], Iter [170/404] Loss: 0.4006\n",
      "Epoch [5/30], Iter [180/404] Loss: 0.1771\n",
      "Epoch [5/30], Iter [190/404] Loss: 0.6229\n",
      "Epoch [5/30], Iter [200/404] Loss: 0.4230\n",
      "Epoch [5/30], Iter [210/404] Loss: 0.4016\n",
      "Epoch [5/30], Iter [220/404] Loss: 0.4186\n",
      "Epoch [5/30], Iter [230/404] Loss: 0.4227\n",
      "Epoch [5/30], Iter [240/404] Loss: 0.3102\n",
      "Epoch [5/30], Iter [250/404] Loss: 0.2878\n",
      "Epoch [5/30], Iter [260/404] Loss: 0.3305\n",
      "Epoch [5/30], Iter [270/404] Loss: 0.2813\n",
      "Epoch [5/30], Iter [280/404] Loss: 0.2413\n",
      "Epoch [5/30], Iter [290/404] Loss: 0.3160\n",
      "Epoch [5/30], Iter [300/404] Loss: 0.3047\n",
      "Epoch [5/30], Iter [310/404] Loss: 0.3547\n",
      "Epoch [5/30], Iter [320/404] Loss: 0.2286\n",
      "Epoch [5/30], Iter [330/404] Loss: 0.3388\n",
      "Epoch [5/30], Iter [340/404] Loss: 0.5202\n",
      "Epoch [5/30], Iter [350/404] Loss: 0.4628\n",
      "Epoch [5/30], Iter [360/404] Loss: 0.1700\n",
      "Epoch [5/30], Iter [370/404] Loss: 0.1598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Iter [380/404] Loss: 0.2726\n",
      "Epoch [5/30], Iter [390/404] Loss: 0.3997\n",
      "Epoch [5/30], Iter [400/404] Loss: 0.1953\n",
      "Epoch [6/30], Iter [10/404] Loss: 0.3132\n",
      "Epoch [6/30], Iter [20/404] Loss: 0.4074\n",
      "Epoch [6/30], Iter [30/404] Loss: 0.2968\n",
      "Epoch [6/30], Iter [40/404] Loss: 0.3605\n",
      "Epoch [6/30], Iter [50/404] Loss: 0.2918\n",
      "Epoch [6/30], Iter [60/404] Loss: 0.2523\n",
      "Epoch [6/30], Iter [70/404] Loss: 0.1459\n",
      "Epoch [6/30], Iter [80/404] Loss: 0.0909\n",
      "Epoch [6/30], Iter [90/404] Loss: 0.1949\n",
      "Epoch [6/30], Iter [100/404] Loss: 0.1241\n",
      "Epoch [6/30], Iter [110/404] Loss: 0.1619\n",
      "Epoch [6/30], Iter [120/404] Loss: 0.4912\n",
      "Epoch [6/30], Iter [130/404] Loss: 0.0934\n",
      "Epoch [6/30], Iter [140/404] Loss: 0.3263\n",
      "Epoch [6/30], Iter [150/404] Loss: 0.1559\n",
      "Epoch [6/30], Iter [160/404] Loss: 0.1276\n",
      "Epoch [6/30], Iter [170/404] Loss: 0.2575\n",
      "Epoch [6/30], Iter [180/404] Loss: 0.2140\n",
      "Epoch [6/30], Iter [190/404] Loss: 0.1830\n",
      "Epoch [6/30], Iter [200/404] Loss: 0.3054\n",
      "Epoch [6/30], Iter [210/404] Loss: 0.3923\n",
      "Epoch [6/30], Iter [220/404] Loss: 0.3035\n",
      "Epoch [6/30], Iter [230/404] Loss: 0.4524\n",
      "Epoch [6/30], Iter [240/404] Loss: 0.4176\n",
      "Epoch [6/30], Iter [250/404] Loss: 0.1120\n",
      "Epoch [6/30], Iter [260/404] Loss: 0.1928\n",
      "Epoch [6/30], Iter [270/404] Loss: 0.1845\n",
      "Epoch [6/30], Iter [280/404] Loss: 0.3963\n",
      "Epoch [6/30], Iter [290/404] Loss: 0.2346\n",
      "Epoch [6/30], Iter [300/404] Loss: 0.4335\n",
      "Epoch [6/30], Iter [310/404] Loss: 0.2421\n",
      "Epoch [6/30], Iter [320/404] Loss: 0.3897\n",
      "Epoch [6/30], Iter [330/404] Loss: 0.0664\n",
      "Epoch [6/30], Iter [340/404] Loss: 0.3799\n",
      "Epoch [6/30], Iter [350/404] Loss: 0.0569\n",
      "Epoch [6/30], Iter [360/404] Loss: 0.1890\n",
      "Epoch [6/30], Iter [370/404] Loss: 0.1763\n",
      "Epoch [6/30], Iter [380/404] Loss: 0.1418\n",
      "Epoch [6/30], Iter [390/404] Loss: 0.2805\n",
      "Epoch [6/30], Iter [400/404] Loss: 0.3091\n",
      "Epoch [7/30], Iter [10/404] Loss: 0.3656\n",
      "Epoch [7/30], Iter [20/404] Loss: 0.1999\n",
      "Epoch [7/30], Iter [30/404] Loss: 0.3061\n",
      "Epoch [7/30], Iter [40/404] Loss: 0.3120\n",
      "Epoch [7/30], Iter [50/404] Loss: 0.0839\n",
      "Epoch [7/30], Iter [60/404] Loss: 0.2454\n",
      "Epoch [7/30], Iter [70/404] Loss: 0.0861\n",
      "Epoch [7/30], Iter [80/404] Loss: 0.2815\n",
      "Epoch [7/30], Iter [90/404] Loss: 0.1046\n",
      "Epoch [7/30], Iter [100/404] Loss: 0.1413\n",
      "Epoch [7/30], Iter [110/404] Loss: 0.4339\n",
      "Epoch [7/30], Iter [120/404] Loss: 0.4620\n",
      "Epoch [7/30], Iter [130/404] Loss: 0.2650\n",
      "Epoch [7/30], Iter [140/404] Loss: 0.2539\n",
      "Epoch [7/30], Iter [150/404] Loss: 0.4734\n",
      "Epoch [7/30], Iter [160/404] Loss: 0.1866\n",
      "Epoch [7/30], Iter [170/404] Loss: 0.1679\n",
      "Epoch [7/30], Iter [180/404] Loss: 0.3703\n",
      "Epoch [7/30], Iter [190/404] Loss: 0.3698\n",
      "Epoch [7/30], Iter [200/404] Loss: 0.0715\n",
      "Epoch [7/30], Iter [210/404] Loss: 0.1469\n",
      "Epoch [7/30], Iter [220/404] Loss: 0.2950\n",
      "Epoch [7/30], Iter [230/404] Loss: 0.1218\n",
      "Epoch [7/30], Iter [240/404] Loss: 0.0411\n",
      "Epoch [7/30], Iter [250/404] Loss: 0.1282\n",
      "Epoch [7/30], Iter [260/404] Loss: 0.4887\n",
      "Epoch [7/30], Iter [270/404] Loss: 0.1153\n",
      "Epoch [7/30], Iter [280/404] Loss: 0.1325\n",
      "Epoch [7/30], Iter [290/404] Loss: 0.1243\n",
      "Epoch [7/30], Iter [300/404] Loss: 0.1405\n",
      "Epoch [7/30], Iter [310/404] Loss: 0.3158\n",
      "Epoch [7/30], Iter [320/404] Loss: 0.0693\n",
      "Epoch [7/30], Iter [330/404] Loss: 0.1396\n",
      "Epoch [7/30], Iter [340/404] Loss: 0.2522\n",
      "Epoch [7/30], Iter [350/404] Loss: 0.1531\n",
      "Epoch [7/30], Iter [360/404] Loss: 0.1480\n",
      "Epoch [7/30], Iter [370/404] Loss: 0.3475\n",
      "Epoch [7/30], Iter [380/404] Loss: 0.3871\n",
      "Epoch [7/30], Iter [390/404] Loss: 0.0988\n",
      "Epoch [7/30], Iter [400/404] Loss: 0.1946\n",
      "Epoch [8/30], Iter [10/404] Loss: 0.1917\n",
      "Epoch [8/30], Iter [20/404] Loss: 0.1231\n",
      "Epoch [8/30], Iter [30/404] Loss: 0.1450\n",
      "Epoch [8/30], Iter [40/404] Loss: 0.0685\n",
      "Epoch [8/30], Iter [50/404] Loss: 0.2172\n",
      "Epoch [8/30], Iter [60/404] Loss: 0.0687\n",
      "Epoch [8/30], Iter [70/404] Loss: 0.1156\n",
      "Epoch [8/30], Iter [80/404] Loss: 0.0493\n",
      "Epoch [8/30], Iter [90/404] Loss: 0.2328\n",
      "Epoch [8/30], Iter [100/404] Loss: 0.1179\n",
      "Epoch [8/30], Iter [110/404] Loss: 0.3405\n",
      "Epoch [8/30], Iter [120/404] Loss: 0.2136\n",
      "Epoch [8/30], Iter [130/404] Loss: 0.1598\n",
      "Epoch [8/30], Iter [140/404] Loss: 0.3437\n",
      "Epoch [8/30], Iter [150/404] Loss: 0.0639\n",
      "Epoch [8/30], Iter [160/404] Loss: 0.2028\n",
      "Epoch [8/30], Iter [170/404] Loss: 0.4843\n",
      "Epoch [8/30], Iter [180/404] Loss: 0.0391\n",
      "Epoch [8/30], Iter [190/404] Loss: 0.1687\n",
      "Epoch [8/30], Iter [200/404] Loss: 0.2056\n",
      "Epoch [8/30], Iter [210/404] Loss: 0.1076\n",
      "Epoch [8/30], Iter [220/404] Loss: 0.2514\n",
      "Epoch [8/30], Iter [230/404] Loss: 0.1216\n",
      "Epoch [8/30], Iter [240/404] Loss: 0.1004\n",
      "Epoch [8/30], Iter [250/404] Loss: 0.0313\n",
      "Epoch [8/30], Iter [260/404] Loss: 0.3245\n",
      "Epoch [8/30], Iter [270/404] Loss: 0.1503\n",
      "Epoch [8/30], Iter [280/404] Loss: 0.0767\n",
      "Epoch [8/30], Iter [290/404] Loss: 0.3102\n",
      "Epoch [8/30], Iter [300/404] Loss: 0.0338\n",
      "Epoch [8/30], Iter [310/404] Loss: 0.2867\n",
      "Epoch [8/30], Iter [320/404] Loss: 0.1241\n",
      "Epoch [8/30], Iter [330/404] Loss: 0.0883\n",
      "Epoch [8/30], Iter [340/404] Loss: 0.2094\n",
      "Epoch [8/30], Iter [350/404] Loss: 0.1634\n",
      "Epoch [8/30], Iter [360/404] Loss: 0.1857\n",
      "Epoch [8/30], Iter [370/404] Loss: 0.1870\n",
      "Epoch [8/30], Iter [380/404] Loss: 0.0657\n",
      "Epoch [8/30], Iter [390/404] Loss: 0.0112\n",
      "Epoch [8/30], Iter [400/404] Loss: 0.2082\n",
      "Epoch [9/30], Iter [10/404] Loss: 0.2226\n",
      "Epoch [9/30], Iter [20/404] Loss: 0.1675\n",
      "Epoch [9/30], Iter [30/404] Loss: 0.0139\n",
      "Epoch [9/30], Iter [40/404] Loss: 0.1669\n",
      "Epoch [9/30], Iter [50/404] Loss: 0.0400\n",
      "Epoch [9/30], Iter [60/404] Loss: 0.1796\n",
      "Epoch [9/30], Iter [70/404] Loss: 0.1449\n",
      "Epoch [9/30], Iter [80/404] Loss: 0.1796\n",
      "Epoch [9/30], Iter [90/404] Loss: 0.0445\n",
      "Epoch [9/30], Iter [100/404] Loss: 0.1007\n",
      "Epoch [9/30], Iter [110/404] Loss: 0.1405\n",
      "Epoch [9/30], Iter [120/404] Loss: 0.1418\n",
      "Epoch [9/30], Iter [130/404] Loss: 0.1005\n",
      "Epoch [9/30], Iter [140/404] Loss: 0.0090\n",
      "Epoch [9/30], Iter [150/404] Loss: 0.1532\n",
      "Epoch [9/30], Iter [160/404] Loss: 0.0077\n",
      "Epoch [9/30], Iter [170/404] Loss: 0.1151\n",
      "Epoch [9/30], Iter [180/404] Loss: 0.2587\n",
      "Epoch [9/30], Iter [190/404] Loss: 0.0600\n",
      "Epoch [9/30], Iter [200/404] Loss: 0.0030\n",
      "Epoch [9/30], Iter [210/404] Loss: 0.1591\n",
      "Epoch [9/30], Iter [220/404] Loss: 0.1235\n",
      "Epoch [9/30], Iter [230/404] Loss: 0.0398\n",
      "Epoch [9/30], Iter [240/404] Loss: 0.0013\n",
      "Epoch [9/30], Iter [250/404] Loss: 0.2041\n",
      "Epoch [9/30], Iter [260/404] Loss: 0.0470\n",
      "Epoch [9/30], Iter [270/404] Loss: 0.0474\n",
      "Epoch [9/30], Iter [280/404] Loss: 0.3493\n",
      "Epoch [9/30], Iter [290/404] Loss: 0.1105\n",
      "Epoch [9/30], Iter [300/404] Loss: 0.0484\n",
      "Epoch [9/30], Iter [310/404] Loss: 0.0197\n",
      "Epoch [9/30], Iter [320/404] Loss: 0.0326\n",
      "Epoch [9/30], Iter [330/404] Loss: 0.0735\n",
      "Epoch [9/30], Iter [340/404] Loss: 0.1505\n",
      "Epoch [9/30], Iter [350/404] Loss: 0.1763\n",
      "Epoch [9/30], Iter [360/404] Loss: 0.1930\n",
      "Epoch [9/30], Iter [370/404] Loss: 0.1605\n",
      "Epoch [9/30], Iter [380/404] Loss: 0.0418\n",
      "Epoch [9/30], Iter [390/404] Loss: 0.1183\n",
      "Epoch [9/30], Iter [400/404] Loss: 0.0051\n",
      "Epoch [10/30], Iter [10/404] Loss: 0.1632\n",
      "Epoch [10/30], Iter [20/404] Loss: 0.0552\n",
      "Epoch [10/30], Iter [30/404] Loss: 0.0061\n",
      "Epoch [10/30], Iter [40/404] Loss: 0.0182\n",
      "Epoch [10/30], Iter [50/404] Loss: 0.0516\n",
      "Epoch [10/30], Iter [60/404] Loss: 0.2425\n",
      "Epoch [10/30], Iter [70/404] Loss: 0.1825\n",
      "Epoch [10/30], Iter [80/404] Loss: 0.0944\n",
      "Epoch [10/30], Iter [90/404] Loss: 0.1093\n",
      "Epoch [10/30], Iter [100/404] Loss: 0.2537\n",
      "Epoch [10/30], Iter [110/404] Loss: 0.0231\n",
      "Epoch [10/30], Iter [120/404] Loss: 0.1018\n",
      "Epoch [10/30], Iter [130/404] Loss: 0.0274\n",
      "Epoch [10/30], Iter [140/404] Loss: 0.0510\n",
      "Epoch [10/30], Iter [150/404] Loss: 0.1339\n",
      "Epoch [10/30], Iter [160/404] Loss: 0.2126\n",
      "Epoch [10/30], Iter [170/404] Loss: 0.1505\n",
      "Epoch [10/30], Iter [180/404] Loss: 0.1682\n",
      "Epoch [10/30], Iter [190/404] Loss: 0.1412\n",
      "Epoch [10/30], Iter [200/404] Loss: 0.0260\n",
      "Epoch [10/30], Iter [210/404] Loss: 0.0822\n",
      "Epoch [10/30], Iter [220/404] Loss: 0.1097\n",
      "Epoch [10/30], Iter [230/404] Loss: 0.0904\n",
      "Epoch [10/30], Iter [240/404] Loss: 0.1116\n",
      "Epoch [10/30], Iter [250/404] Loss: 0.0109\n",
      "Epoch [10/30], Iter [260/404] Loss: 0.0304\n",
      "Epoch [10/30], Iter [270/404] Loss: 0.1034\n",
      "Epoch [10/30], Iter [280/404] Loss: 0.0231\n",
      "Epoch [10/30], Iter [290/404] Loss: 0.0163\n",
      "Epoch [10/30], Iter [300/404] Loss: 0.1788\n",
      "Epoch [10/30], Iter [310/404] Loss: 0.1276\n",
      "Epoch [10/30], Iter [320/404] Loss: 0.1229\n",
      "Epoch [10/30], Iter [330/404] Loss: 0.1529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Iter [340/404] Loss: 0.0577\n",
      "Epoch [10/30], Iter [350/404] Loss: 0.1162\n",
      "Epoch [10/30], Iter [360/404] Loss: 0.1089\n",
      "Epoch [10/30], Iter [370/404] Loss: 0.1600\n",
      "Epoch [10/30], Iter [380/404] Loss: 0.0014\n",
      "Epoch [10/30], Iter [390/404] Loss: 0.0289\n",
      "Epoch [10/30], Iter [400/404] Loss: 0.0521\n",
      "Epoch [11/30], Iter [10/404] Loss: 0.0461\n",
      "Epoch [11/30], Iter [20/404] Loss: 0.2173\n",
      "Epoch [11/30], Iter [30/404] Loss: 0.0454\n",
      "Epoch [11/30], Iter [40/404] Loss: 0.1530\n",
      "Epoch [11/30], Iter [50/404] Loss: 0.1740\n",
      "Epoch [11/30], Iter [60/404] Loss: 0.0174\n",
      "Epoch [11/30], Iter [70/404] Loss: 0.0123\n",
      "Epoch [11/30], Iter [80/404] Loss: 0.0174\n",
      "Epoch [11/30], Iter [90/404] Loss: 0.0982\n",
      "Epoch [11/30], Iter [100/404] Loss: 0.0136\n",
      "Epoch [11/30], Iter [110/404] Loss: 0.0686\n",
      "Epoch [11/30], Iter [120/404] Loss: 0.0155\n",
      "Epoch [11/30], Iter [130/404] Loss: 0.3826\n",
      "Epoch [11/30], Iter [140/404] Loss: 0.1463\n",
      "Epoch [11/30], Iter [150/404] Loss: 0.0511\n",
      "Epoch [11/30], Iter [160/404] Loss: 0.0732\n",
      "Epoch [11/30], Iter [170/404] Loss: 0.1018\n",
      "Epoch [11/30], Iter [180/404] Loss: 0.0098\n",
      "Epoch [11/30], Iter [190/404] Loss: 0.0007\n",
      "Epoch [11/30], Iter [200/404] Loss: 0.0942\n",
      "Epoch [11/30], Iter [210/404] Loss: 0.2437\n",
      "Epoch [11/30], Iter [220/404] Loss: 0.0679\n",
      "Epoch [11/30], Iter [230/404] Loss: 0.0131\n",
      "Epoch [11/30], Iter [240/404] Loss: 0.1170\n",
      "Epoch [11/30], Iter [250/404] Loss: 0.0882\n",
      "Epoch [11/30], Iter [260/404] Loss: 0.0103\n",
      "Epoch [11/30], Iter [270/404] Loss: 0.0510\n",
      "Epoch [11/30], Iter [280/404] Loss: 0.2759\n",
      "Epoch [11/30], Iter [290/404] Loss: 0.0392\n",
      "Epoch [11/30], Iter [300/404] Loss: 0.0482\n",
      "Epoch [11/30], Iter [310/404] Loss: 0.3186\n",
      "Epoch [11/30], Iter [320/404] Loss: 0.2205\n",
      "Epoch [11/30], Iter [330/404] Loss: 0.0277\n",
      "Epoch [11/30], Iter [340/404] Loss: 0.0642\n",
      "Epoch [11/30], Iter [350/404] Loss: 0.0659\n",
      "Epoch [11/30], Iter [360/404] Loss: 0.0865\n",
      "Epoch [11/30], Iter [370/404] Loss: 0.2390\n",
      "Epoch [11/30], Iter [380/404] Loss: 0.0746\n",
      "Epoch [11/30], Iter [390/404] Loss: 0.0410\n",
      "Epoch [11/30], Iter [400/404] Loss: 0.3818\n",
      "Epoch [12/30], Iter [10/404] Loss: 0.1194\n",
      "Epoch [12/30], Iter [20/404] Loss: 0.0079\n",
      "Epoch [12/30], Iter [30/404] Loss: 0.0350\n",
      "Epoch [12/30], Iter [40/404] Loss: 0.0111\n",
      "Epoch [12/30], Iter [50/404] Loss: 0.0602\n",
      "Epoch [12/30], Iter [60/404] Loss: 0.0967\n",
      "Epoch [12/30], Iter [70/404] Loss: 0.0515\n",
      "Epoch [12/30], Iter [80/404] Loss: 0.2175\n",
      "Epoch [12/30], Iter [90/404] Loss: 0.1167\n",
      "Epoch [12/30], Iter [100/404] Loss: 0.1578\n",
      "Epoch [12/30], Iter [110/404] Loss: 0.1424\n",
      "Epoch [12/30], Iter [120/404] Loss: 0.1765\n",
      "Epoch [12/30], Iter [130/404] Loss: 0.0608\n",
      "Epoch [12/30], Iter [140/404] Loss: 0.2336\n",
      "Epoch [12/30], Iter [150/404] Loss: 0.0170\n",
      "Epoch [12/30], Iter [160/404] Loss: 0.0442\n",
      "Epoch [12/30], Iter [170/404] Loss: 0.0218\n",
      "Epoch [12/30], Iter [180/404] Loss: 0.0478\n",
      "Epoch [12/30], Iter [190/404] Loss: 0.0178\n",
      "Epoch [12/30], Iter [200/404] Loss: 0.0764\n",
      "Epoch [12/30], Iter [210/404] Loss: 0.0720\n",
      "Epoch [12/30], Iter [220/404] Loss: 0.0746\n",
      "Epoch [12/30], Iter [230/404] Loss: 0.1220\n",
      "Epoch [12/30], Iter [240/404] Loss: 0.0813\n",
      "Epoch [12/30], Iter [250/404] Loss: 0.0483\n",
      "Epoch [12/30], Iter [260/404] Loss: 0.0278\n",
      "Epoch [12/30], Iter [270/404] Loss: 0.1522\n",
      "Epoch [12/30], Iter [280/404] Loss: 0.1357\n",
      "Epoch [12/30], Iter [290/404] Loss: 0.1414\n",
      "Epoch [12/30], Iter [300/404] Loss: 0.1416\n",
      "Epoch [12/30], Iter [310/404] Loss: 0.1354\n",
      "Epoch [12/30], Iter [320/404] Loss: 0.0354\n",
      "Epoch [12/30], Iter [330/404] Loss: 0.0887\n",
      "Epoch [12/30], Iter [340/404] Loss: 0.2731\n",
      "Epoch [12/30], Iter [350/404] Loss: 0.1342\n",
      "Epoch [12/30], Iter [360/404] Loss: 0.1129\n",
      "Epoch [12/30], Iter [370/404] Loss: 0.1844\n",
      "Epoch [12/30], Iter [380/404] Loss: 0.1685\n",
      "Epoch [12/30], Iter [390/404] Loss: 0.0390\n",
      "Epoch [12/30], Iter [400/404] Loss: 0.0421\n",
      "Epoch [13/30], Iter [10/404] Loss: 0.1166\n",
      "Epoch [13/30], Iter [20/404] Loss: 0.4177\n",
      "Epoch [13/30], Iter [30/404] Loss: 0.0441\n",
      "Epoch [13/30], Iter [40/404] Loss: 0.0230\n",
      "Epoch [13/30], Iter [50/404] Loss: 0.0458\n",
      "Epoch [13/30], Iter [60/404] Loss: 0.3397\n",
      "Epoch [13/30], Iter [70/404] Loss: 0.1358\n",
      "Epoch [13/30], Iter [80/404] Loss: 0.0231\n",
      "Epoch [13/30], Iter [90/404] Loss: 0.0777\n",
      "Epoch [13/30], Iter [100/404] Loss: 0.1386\n",
      "Epoch [13/30], Iter [110/404] Loss: 0.0733\n",
      "Epoch [13/30], Iter [120/404] Loss: 0.0679\n",
      "Epoch [13/30], Iter [130/404] Loss: 0.1249\n",
      "Epoch [13/30], Iter [140/404] Loss: 0.0225\n",
      "Epoch [13/30], Iter [150/404] Loss: 0.0017\n",
      "Epoch [13/30], Iter [160/404] Loss: 0.1024\n",
      "Epoch [13/30], Iter [170/404] Loss: 0.1364\n",
      "Epoch [13/30], Iter [180/404] Loss: 0.0634\n",
      "Epoch [13/30], Iter [190/404] Loss: 0.0110\n",
      "Epoch [13/30], Iter [200/404] Loss: 0.0660\n",
      "Epoch [13/30], Iter [210/404] Loss: 0.0161\n",
      "Epoch [13/30], Iter [220/404] Loss: 0.0788\n",
      "Epoch [13/30], Iter [230/404] Loss: 0.0273\n",
      "Epoch [13/30], Iter [240/404] Loss: 0.0393\n",
      "Epoch [13/30], Iter [250/404] Loss: 0.0908\n",
      "Epoch [13/30], Iter [260/404] Loss: 0.0194\n",
      "Epoch [13/30], Iter [270/404] Loss: 0.0212\n",
      "Epoch [13/30], Iter [280/404] Loss: 0.0057\n",
      "Epoch [13/30], Iter [290/404] Loss: 0.0006\n",
      "Epoch [13/30], Iter [300/404] Loss: 0.1021\n",
      "Epoch [13/30], Iter [310/404] Loss: 0.0688\n",
      "Epoch [13/30], Iter [320/404] Loss: 0.0037\n",
      "Epoch [13/30], Iter [330/404] Loss: 0.0003\n",
      "Epoch [13/30], Iter [340/404] Loss: 0.0552\n",
      "Epoch [13/30], Iter [350/404] Loss: 0.1939\n",
      "Epoch [13/30], Iter [360/404] Loss: 0.0042\n",
      "Epoch [13/30], Iter [370/404] Loss: 0.0254\n",
      "Epoch [13/30], Iter [380/404] Loss: 0.1488\n",
      "Epoch [13/30], Iter [390/404] Loss: 0.0263\n",
      "Epoch [13/30], Iter [400/404] Loss: 0.2674\n",
      "Epoch [14/30], Iter [10/404] Loss: 0.0303\n",
      "Epoch [14/30], Iter [20/404] Loss: 0.0223\n",
      "Epoch [14/30], Iter [30/404] Loss: 0.0175\n",
      "Epoch [14/30], Iter [40/404] Loss: 0.1611\n",
      "Epoch [14/30], Iter [50/404] Loss: 0.1702\n",
      "Epoch [14/30], Iter [60/404] Loss: 0.0499\n",
      "Epoch [14/30], Iter [70/404] Loss: 0.2891\n",
      "Epoch [14/30], Iter [80/404] Loss: 0.0128\n",
      "Epoch [14/30], Iter [90/404] Loss: 0.0077\n",
      "Epoch [14/30], Iter [100/404] Loss: 0.0681\n",
      "Epoch [14/30], Iter [110/404] Loss: 0.2559\n",
      "Epoch [14/30], Iter [120/404] Loss: 0.0975\n",
      "Epoch [14/30], Iter [130/404] Loss: 0.0036\n",
      "Epoch [14/30], Iter [140/404] Loss: 0.0074\n",
      "Epoch [14/30], Iter [150/404] Loss: 0.0529\n",
      "Epoch [14/30], Iter [160/404] Loss: 0.1437\n",
      "Epoch [14/30], Iter [170/404] Loss: 0.0049\n",
      "Epoch [14/30], Iter [180/404] Loss: 0.0063\n",
      "Epoch [14/30], Iter [190/404] Loss: 0.0971\n",
      "Epoch [14/30], Iter [200/404] Loss: 0.0771\n",
      "Epoch [14/30], Iter [210/404] Loss: 0.0513\n",
      "Epoch [14/30], Iter [220/404] Loss: 0.0199\n",
      "Epoch [14/30], Iter [230/404] Loss: 0.0011\n",
      "Epoch [14/30], Iter [240/404] Loss: 0.0102\n",
      "Epoch [14/30], Iter [250/404] Loss: 0.0907\n",
      "Epoch [14/30], Iter [260/404] Loss: 0.0197\n",
      "Epoch [14/30], Iter [270/404] Loss: 0.1428\n",
      "Epoch [14/30], Iter [280/404] Loss: 0.1236\n",
      "Epoch [14/30], Iter [290/404] Loss: 0.0366\n",
      "Epoch [14/30], Iter [300/404] Loss: 0.0731\n",
      "Epoch [14/30], Iter [310/404] Loss: 0.2047\n",
      "Epoch [14/30], Iter [320/404] Loss: 0.0250\n",
      "Epoch [14/30], Iter [330/404] Loss: 0.0293\n",
      "Epoch [14/30], Iter [340/404] Loss: 0.2054\n",
      "Epoch [14/30], Iter [350/404] Loss: 0.0816\n",
      "Epoch [14/30], Iter [360/404] Loss: 0.2360\n",
      "Epoch [14/30], Iter [370/404] Loss: 0.0290\n",
      "Epoch [14/30], Iter [380/404] Loss: 0.0349\n",
      "Epoch [14/30], Iter [390/404] Loss: 0.0017\n",
      "Epoch [14/30], Iter [400/404] Loss: 0.0664\n",
      "Epoch [15/30], Iter [10/404] Loss: 0.0134\n",
      "Epoch [15/30], Iter [20/404] Loss: 0.1252\n",
      "Epoch [15/30], Iter [30/404] Loss: 0.0047\n",
      "Epoch [15/30], Iter [40/404] Loss: 0.1031\n",
      "Epoch [15/30], Iter [50/404] Loss: 0.1888\n",
      "Epoch [15/30], Iter [60/404] Loss: 0.0981\n",
      "Epoch [15/30], Iter [70/404] Loss: 0.0341\n",
      "Epoch [15/30], Iter [80/404] Loss: 0.0017\n",
      "Epoch [15/30], Iter [90/404] Loss: 0.0019\n",
      "Epoch [15/30], Iter [100/404] Loss: 0.0430\n",
      "Epoch [15/30], Iter [110/404] Loss: 0.1607\n",
      "Epoch [15/30], Iter [120/404] Loss: 0.0045\n",
      "Epoch [15/30], Iter [130/404] Loss: 0.0088\n",
      "Epoch [15/30], Iter [140/404] Loss: 0.0168\n",
      "Epoch [15/30], Iter [150/404] Loss: 0.0237\n",
      "Epoch [15/30], Iter [160/404] Loss: 0.0259\n",
      "Epoch [15/30], Iter [170/404] Loss: 0.0167\n",
      "Epoch [15/30], Iter [180/404] Loss: 0.0719\n",
      "Epoch [15/30], Iter [190/404] Loss: 0.0085\n",
      "Epoch [15/30], Iter [200/404] Loss: 0.0922\n",
      "Epoch [15/30], Iter [210/404] Loss: 0.0177\n",
      "Epoch [15/30], Iter [220/404] Loss: 0.0008\n",
      "Epoch [15/30], Iter [230/404] Loss: 0.0202\n",
      "Epoch [15/30], Iter [240/404] Loss: 0.0020\n",
      "Epoch [15/30], Iter [250/404] Loss: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Iter [260/404] Loss: 0.0880\n",
      "Epoch [15/30], Iter [270/404] Loss: 0.0147\n",
      "Epoch [15/30], Iter [280/404] Loss: 0.1592\n",
      "Epoch [15/30], Iter [290/404] Loss: 0.1770\n",
      "Epoch [15/30], Iter [300/404] Loss: 0.1579\n",
      "Epoch [15/30], Iter [310/404] Loss: 0.0012\n",
      "Epoch [15/30], Iter [320/404] Loss: 0.1526\n",
      "Epoch [15/30], Iter [330/404] Loss: 0.0341\n",
      "Epoch [15/30], Iter [340/404] Loss: 0.0729\n",
      "Epoch [15/30], Iter [350/404] Loss: 0.0087\n",
      "Epoch [15/30], Iter [360/404] Loss: 0.0186\n",
      "Epoch [15/30], Iter [370/404] Loss: 0.0115\n",
      "Epoch [15/30], Iter [380/404] Loss: 0.0066\n",
      "Epoch [15/30], Iter [390/404] Loss: 0.0041\n",
      "Epoch [15/30], Iter [400/404] Loss: 0.0476\n",
      "Epoch [16/30], Iter [10/404] Loss: 0.0000\n",
      "Epoch [16/30], Iter [20/404] Loss: 0.0016\n",
      "Epoch [16/30], Iter [30/404] Loss: 0.0129\n",
      "Epoch [16/30], Iter [40/404] Loss: 0.0006\n",
      "Epoch [16/30], Iter [50/404] Loss: 0.0765\n",
      "Epoch [16/30], Iter [60/404] Loss: 0.0161\n",
      "Epoch [16/30], Iter [70/404] Loss: 0.0068\n",
      "Epoch [16/30], Iter [80/404] Loss: 0.0294\n",
      "Epoch [16/30], Iter [90/404] Loss: 0.0011\n",
      "Epoch [16/30], Iter [100/404] Loss: 0.0265\n",
      "Epoch [16/30], Iter [110/404] Loss: 0.1923\n",
      "Epoch [16/30], Iter [120/404] Loss: 0.0040\n",
      "Epoch [16/30], Iter [130/404] Loss: 0.0011\n",
      "Epoch [16/30], Iter [140/404] Loss: 0.0012\n",
      "Epoch [16/30], Iter [150/404] Loss: 0.0003\n",
      "Epoch [16/30], Iter [160/404] Loss: 0.0445\n",
      "Epoch [16/30], Iter [170/404] Loss: 0.0000\n",
      "Epoch [16/30], Iter [180/404] Loss: 0.0962\n",
      "Epoch [16/30], Iter [190/404] Loss: 0.0854\n",
      "Epoch [16/30], Iter [200/404] Loss: 0.0313\n",
      "Epoch [16/30], Iter [210/404] Loss: 0.0148\n",
      "Epoch [16/30], Iter [220/404] Loss: 0.0148\n",
      "Epoch [16/30], Iter [230/404] Loss: 0.0600\n",
      "Epoch [16/30], Iter [240/404] Loss: 0.1516\n",
      "Epoch [16/30], Iter [250/404] Loss: 0.0027\n",
      "Epoch [16/30], Iter [260/404] Loss: 0.0784\n",
      "Epoch [16/30], Iter [270/404] Loss: 0.3204\n",
      "Epoch [16/30], Iter [280/404] Loss: 0.0825\n",
      "Epoch [16/30], Iter [290/404] Loss: 0.0142\n",
      "Epoch [16/30], Iter [300/404] Loss: 0.0109\n",
      "Epoch [16/30], Iter [310/404] Loss: 0.0942\n",
      "Epoch [16/30], Iter [320/404] Loss: 0.0042\n",
      "Epoch [16/30], Iter [330/404] Loss: 0.2026\n",
      "Epoch [16/30], Iter [340/404] Loss: 0.0008\n",
      "Epoch [16/30], Iter [350/404] Loss: 0.0067\n",
      "Epoch [16/30], Iter [360/404] Loss: 0.0310\n",
      "Epoch [16/30], Iter [370/404] Loss: 0.1556\n",
      "Epoch [16/30], Iter [380/404] Loss: 0.0723\n",
      "Epoch [16/30], Iter [390/404] Loss: 0.0067\n",
      "Epoch [16/30], Iter [400/404] Loss: 0.1124\n",
      "Epoch [17/30], Iter [10/404] Loss: 0.0028\n",
      "Epoch [17/30], Iter [20/404] Loss: 0.1454\n",
      "Epoch [17/30], Iter [30/404] Loss: 0.3637\n",
      "Epoch [17/30], Iter [40/404] Loss: 0.3849\n",
      "Epoch [17/30], Iter [50/404] Loss: 0.0803\n",
      "Epoch [17/30], Iter [60/404] Loss: 0.3183\n",
      "Epoch [17/30], Iter [70/404] Loss: 0.0102\n",
      "Epoch [17/30], Iter [80/404] Loss: 0.1360\n",
      "Epoch [17/30], Iter [90/404] Loss: 0.0884\n",
      "Epoch [17/30], Iter [100/404] Loss: 0.0109\n",
      "Epoch [17/30], Iter [110/404] Loss: 0.2392\n",
      "Epoch [17/30], Iter [120/404] Loss: 0.1315\n",
      "Epoch [17/30], Iter [130/404] Loss: 0.0335\n",
      "Epoch [17/30], Iter [140/404] Loss: 0.0156\n",
      "Epoch [17/30], Iter [150/404] Loss: 0.1081\n",
      "Epoch [17/30], Iter [160/404] Loss: 0.0086\n",
      "Epoch [17/30], Iter [170/404] Loss: 0.0262\n",
      "Epoch [17/30], Iter [180/404] Loss: 0.0960\n",
      "Epoch [17/30], Iter [190/404] Loss: 0.0592\n",
      "Epoch [17/30], Iter [200/404] Loss: 0.0035\n",
      "Epoch [17/30], Iter [210/404] Loss: 0.1177\n",
      "Epoch [17/30], Iter [220/404] Loss: 0.0155\n",
      "Epoch [17/30], Iter [230/404] Loss: 0.2159\n",
      "Epoch [17/30], Iter [240/404] Loss: 0.0260\n",
      "Epoch [17/30], Iter [250/404] Loss: 0.0163\n",
      "Epoch [17/30], Iter [260/404] Loss: 0.0037\n",
      "Epoch [17/30], Iter [270/404] Loss: 0.0024\n",
      "Epoch [17/30], Iter [280/404] Loss: 0.0830\n",
      "Epoch [17/30], Iter [290/404] Loss: 0.0529\n",
      "Epoch [17/30], Iter [300/404] Loss: 0.0043\n",
      "Epoch [17/30], Iter [310/404] Loss: 0.0008\n",
      "Epoch [17/30], Iter [320/404] Loss: 0.0008\n",
      "Epoch [17/30], Iter [330/404] Loss: 0.0442\n",
      "Epoch [17/30], Iter [340/404] Loss: 0.0292\n",
      "Epoch [17/30], Iter [350/404] Loss: 0.0331\n",
      "Epoch [17/30], Iter [360/404] Loss: 0.1510\n",
      "Epoch [17/30], Iter [370/404] Loss: 0.0172\n",
      "Epoch [17/30], Iter [380/404] Loss: 0.1012\n",
      "Epoch [17/30], Iter [390/404] Loss: 0.0128\n",
      "Epoch [17/30], Iter [400/404] Loss: 0.0114\n",
      "Epoch [18/30], Iter [10/404] Loss: 0.0197\n",
      "Epoch [18/30], Iter [20/404] Loss: 0.0020\n",
      "Epoch [18/30], Iter [30/404] Loss: 0.0892\n",
      "Epoch [18/30], Iter [40/404] Loss: 0.0572\n",
      "Epoch [18/30], Iter [50/404] Loss: 0.0001\n",
      "Epoch [18/30], Iter [60/404] Loss: 0.0080\n",
      "Epoch [18/30], Iter [70/404] Loss: 0.0006\n",
      "Epoch [18/30], Iter [80/404] Loss: 0.0456\n",
      "Epoch [18/30], Iter [90/404] Loss: 0.0028\n",
      "Epoch [18/30], Iter [100/404] Loss: 0.0269\n",
      "Epoch [18/30], Iter [110/404] Loss: 0.1538\n",
      "Epoch [18/30], Iter [120/404] Loss: 0.0039\n",
      "Epoch [18/30], Iter [130/404] Loss: 0.0735\n",
      "Epoch [18/30], Iter [140/404] Loss: 0.1155\n",
      "Epoch [18/30], Iter [150/404] Loss: 0.0999\n",
      "Epoch [18/30], Iter [160/404] Loss: 0.0929\n",
      "Epoch [18/30], Iter [170/404] Loss: 0.0073\n",
      "Epoch [18/30], Iter [180/404] Loss: 0.0550\n",
      "Epoch [18/30], Iter [190/404] Loss: 0.0364\n",
      "Epoch [18/30], Iter [200/404] Loss: 0.0016\n",
      "Epoch [18/30], Iter [210/404] Loss: 0.2159\n",
      "Epoch [18/30], Iter [220/404] Loss: 0.0091\n",
      "Epoch [18/30], Iter [230/404] Loss: 0.0049\n",
      "Epoch [18/30], Iter [240/404] Loss: 0.0029\n",
      "Epoch [18/30], Iter [250/404] Loss: 0.0056\n",
      "Epoch [18/30], Iter [260/404] Loss: 0.1090\n",
      "Epoch [18/30], Iter [270/404] Loss: 0.0239\n",
      "Epoch [18/30], Iter [280/404] Loss: 0.1345\n",
      "Epoch [18/30], Iter [290/404] Loss: 0.0426\n",
      "Epoch [18/30], Iter [300/404] Loss: 0.0199\n",
      "Epoch [18/30], Iter [310/404] Loss: 0.0753\n",
      "Epoch [18/30], Iter [320/404] Loss: 0.0061\n",
      "Epoch [18/30], Iter [330/404] Loss: 0.0177\n",
      "Epoch [18/30], Iter [340/404] Loss: 0.0608\n",
      "Epoch [18/30], Iter [350/404] Loss: 0.0337\n",
      "Epoch [18/30], Iter [360/404] Loss: 0.1026\n",
      "Epoch [18/30], Iter [370/404] Loss: 0.0027\n",
      "Epoch [18/30], Iter [380/404] Loss: 0.0139\n",
      "Epoch [18/30], Iter [390/404] Loss: 0.0004\n",
      "Epoch [18/30], Iter [400/404] Loss: 0.0051\n",
      "Epoch [19/30], Iter [10/404] Loss: 0.0282\n",
      "Epoch [19/30], Iter [20/404] Loss: 0.0005\n",
      "Epoch [19/30], Iter [30/404] Loss: 0.0072\n",
      "Epoch [19/30], Iter [40/404] Loss: 0.0008\n",
      "Epoch [19/30], Iter [50/404] Loss: 0.0479\n",
      "Epoch [19/30], Iter [60/404] Loss: 0.0011\n",
      "Epoch [19/30], Iter [70/404] Loss: 0.0001\n",
      "Epoch [19/30], Iter [80/404] Loss: 0.0002\n",
      "Epoch [19/30], Iter [90/404] Loss: 0.0733\n",
      "Epoch [19/30], Iter [100/404] Loss: 0.0624\n",
      "Epoch [19/30], Iter [110/404] Loss: 0.0003\n",
      "Epoch [19/30], Iter [120/404] Loss: 0.0876\n",
      "Epoch [19/30], Iter [130/404] Loss: 0.0095\n",
      "Epoch [19/30], Iter [140/404] Loss: 0.1159\n",
      "Epoch [19/30], Iter [150/404] Loss: 0.0184\n",
      "Epoch [19/30], Iter [160/404] Loss: 0.0060\n",
      "Epoch [19/30], Iter [170/404] Loss: 0.0102\n",
      "Epoch [19/30], Iter [180/404] Loss: 0.0038\n",
      "Epoch [19/30], Iter [190/404] Loss: 0.0137\n",
      "Epoch [19/30], Iter [200/404] Loss: 0.1455\n",
      "Epoch [19/30], Iter [210/404] Loss: 0.0412\n",
      "Epoch [19/30], Iter [220/404] Loss: 0.0673\n",
      "Epoch [19/30], Iter [230/404] Loss: 0.1121\n",
      "Epoch [19/30], Iter [240/404] Loss: 0.0869\n",
      "Epoch [19/30], Iter [250/404] Loss: 0.0078\n",
      "Epoch [19/30], Iter [260/404] Loss: 0.0303\n",
      "Epoch [19/30], Iter [270/404] Loss: 0.0513\n",
      "Epoch [19/30], Iter [280/404] Loss: 0.0052\n",
      "Epoch [19/30], Iter [290/404] Loss: 0.0225\n",
      "Epoch [19/30], Iter [300/404] Loss: 0.0641\n",
      "Epoch [19/30], Iter [310/404] Loss: 0.0923\n",
      "Epoch [19/30], Iter [320/404] Loss: 0.0070\n",
      "Epoch [19/30], Iter [330/404] Loss: 0.0008\n",
      "Epoch [19/30], Iter [340/404] Loss: 0.0474\n",
      "Epoch [19/30], Iter [350/404] Loss: 0.0041\n",
      "Epoch [19/30], Iter [360/404] Loss: 0.0261\n",
      "Epoch [19/30], Iter [370/404] Loss: 0.0184\n",
      "Epoch [19/30], Iter [380/404] Loss: 0.0024\n",
      "Epoch [19/30], Iter [390/404] Loss: 0.0093\n",
      "Epoch [19/30], Iter [400/404] Loss: 0.0689\n",
      "Epoch [20/30], Iter [10/404] Loss: 0.0003\n",
      "Epoch [20/30], Iter [20/404] Loss: 0.0185\n",
      "Epoch [20/30], Iter [30/404] Loss: 0.0076\n",
      "Epoch [20/30], Iter [40/404] Loss: 0.0364\n",
      "Epoch [20/30], Iter [50/404] Loss: 0.0676\n",
      "Epoch [20/30], Iter [60/404] Loss: 0.0371\n",
      "Epoch [20/30], Iter [70/404] Loss: 0.0022\n",
      "Epoch [20/30], Iter [80/404] Loss: 0.0471\n",
      "Epoch [20/30], Iter [90/404] Loss: 0.0897\n",
      "Epoch [20/30], Iter [100/404] Loss: 0.0626\n",
      "Epoch [20/30], Iter [110/404] Loss: 0.0024\n",
      "Epoch [20/30], Iter [120/404] Loss: 0.0257\n",
      "Epoch [20/30], Iter [130/404] Loss: 0.0040\n",
      "Epoch [20/30], Iter [140/404] Loss: 0.0345\n",
      "Epoch [20/30], Iter [150/404] Loss: 0.0001\n",
      "Epoch [20/30], Iter [160/404] Loss: 0.1033\n",
      "Epoch [20/30], Iter [170/404] Loss: 0.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Iter [180/404] Loss: 0.0017\n",
      "Epoch [20/30], Iter [190/404] Loss: 0.0063\n",
      "Epoch [20/30], Iter [200/404] Loss: 0.0016\n",
      "Epoch [20/30], Iter [210/404] Loss: 0.0000\n",
      "Epoch [20/30], Iter [220/404] Loss: 0.0000\n",
      "Epoch [20/30], Iter [230/404] Loss: 0.0002\n",
      "Epoch [20/30], Iter [240/404] Loss: 0.0167\n",
      "Epoch [20/30], Iter [250/404] Loss: 0.0122\n",
      "Epoch [20/30], Iter [260/404] Loss: 0.0036\n",
      "Epoch [20/30], Iter [270/404] Loss: 0.0091\n",
      "Epoch [20/30], Iter [280/404] Loss: 0.0685\n",
      "Epoch [20/30], Iter [290/404] Loss: 0.0336\n",
      "Epoch [20/30], Iter [300/404] Loss: 0.0012\n",
      "Epoch [20/30], Iter [310/404] Loss: 0.0003\n",
      "Epoch [20/30], Iter [320/404] Loss: 0.0051\n",
      "Epoch [20/30], Iter [330/404] Loss: 0.0005\n",
      "Epoch [20/30], Iter [340/404] Loss: 0.0359\n",
      "Epoch [20/30], Iter [350/404] Loss: 0.0119\n",
      "Epoch [20/30], Iter [360/404] Loss: 0.0044\n",
      "Epoch [20/30], Iter [370/404] Loss: 0.0000\n",
      "Epoch [20/30], Iter [380/404] Loss: 0.0878\n",
      "Epoch [20/30], Iter [390/404] Loss: 0.0431\n",
      "Epoch [20/30], Iter [400/404] Loss: 0.0347\n",
      "Epoch [21/30], Iter [10/404] Loss: 0.0042\n",
      "Epoch [21/30], Iter [20/404] Loss: 0.0247\n",
      "Epoch [21/30], Iter [30/404] Loss: 0.0305\n",
      "Epoch [21/30], Iter [40/404] Loss: 0.0065\n",
      "Epoch [21/30], Iter [50/404] Loss: 0.0297\n",
      "Epoch [21/30], Iter [60/404] Loss: 0.0001\n",
      "Epoch [21/30], Iter [70/404] Loss: 0.0010\n",
      "Epoch [21/30], Iter [80/404] Loss: 0.0222\n",
      "Epoch [21/30], Iter [90/404] Loss: 0.0230\n",
      "Epoch [21/30], Iter [100/404] Loss: 0.1107\n",
      "Epoch [21/30], Iter [110/404] Loss: 0.0000\n",
      "Epoch [21/30], Iter [120/404] Loss: 0.0004\n",
      "Epoch [21/30], Iter [130/404] Loss: 0.0006\n",
      "Epoch [21/30], Iter [140/404] Loss: 0.0020\n",
      "Epoch [21/30], Iter [150/404] Loss: 0.0949\n",
      "Epoch [21/30], Iter [160/404] Loss: 0.1336\n",
      "Epoch [21/30], Iter [170/404] Loss: 0.0008\n",
      "Epoch [21/30], Iter [180/404] Loss: 0.0602\n",
      "Epoch [21/30], Iter [190/404] Loss: 0.0014\n",
      "Epoch [21/30], Iter [200/404] Loss: 0.0003\n",
      "Epoch [21/30], Iter [210/404] Loss: 0.0413\n",
      "Epoch [21/30], Iter [220/404] Loss: 0.2902\n",
      "Epoch [21/30], Iter [230/404] Loss: 0.0949\n",
      "Epoch [21/30], Iter [240/404] Loss: 0.0030\n",
      "Epoch [21/30], Iter [250/404] Loss: 0.0117\n",
      "Epoch [21/30], Iter [260/404] Loss: 0.0178\n",
      "Epoch [21/30], Iter [270/404] Loss: 0.0002\n",
      "Epoch [21/30], Iter [280/404] Loss: 0.0031\n",
      "Epoch [21/30], Iter [290/404] Loss: 0.1659\n",
      "Epoch [21/30], Iter [300/404] Loss: 0.0001\n",
      "Epoch [21/30], Iter [310/404] Loss: 0.0057\n",
      "Epoch [21/30], Iter [320/404] Loss: 0.0006\n",
      "Epoch [21/30], Iter [330/404] Loss: 0.0000\n",
      "Epoch [21/30], Iter [340/404] Loss: 0.0015\n",
      "Epoch [21/30], Iter [350/404] Loss: 0.0883\n",
      "Epoch [21/30], Iter [360/404] Loss: 0.0054\n",
      "Epoch [21/30], Iter [370/404] Loss: 0.0349\n",
      "Epoch [21/30], Iter [380/404] Loss: 0.0522\n",
      "Epoch [21/30], Iter [390/404] Loss: 0.0081\n",
      "Epoch [21/30], Iter [400/404] Loss: 0.0015\n",
      "Epoch [22/30], Iter [10/404] Loss: 0.0234\n",
      "Epoch [22/30], Iter [20/404] Loss: 0.0081\n",
      "Epoch [22/30], Iter [30/404] Loss: 0.0020\n",
      "Epoch [22/30], Iter [40/404] Loss: 0.0017\n",
      "Epoch [22/30], Iter [50/404] Loss: 0.0434\n",
      "Epoch [22/30], Iter [60/404] Loss: 0.0013\n",
      "Epoch [22/30], Iter [70/404] Loss: 0.0010\n",
      "Epoch [22/30], Iter [80/404] Loss: 0.0857\n",
      "Epoch [22/30], Iter [90/404] Loss: 0.0030\n",
      "Epoch [22/30], Iter [100/404] Loss: 0.0165\n",
      "Epoch [22/30], Iter [110/404] Loss: 0.0010\n",
      "Epoch [22/30], Iter [120/404] Loss: 0.0142\n",
      "Epoch [22/30], Iter [130/404] Loss: 0.0001\n",
      "Epoch [22/30], Iter [140/404] Loss: 0.0011\n",
      "Epoch [22/30], Iter [150/404] Loss: 0.0001\n",
      "Epoch [22/30], Iter [160/404] Loss: 0.0240\n",
      "Epoch [22/30], Iter [170/404] Loss: 0.0016\n",
      "Epoch [22/30], Iter [180/404] Loss: 0.0000\n",
      "Epoch [22/30], Iter [190/404] Loss: 0.0767\n",
      "Epoch [22/30], Iter [200/404] Loss: 0.0013\n",
      "Epoch [22/30], Iter [210/404] Loss: 0.1380\n",
      "Epoch [22/30], Iter [220/404] Loss: 0.0720\n",
      "Epoch [22/30], Iter [230/404] Loss: 0.0033\n",
      "Epoch [22/30], Iter [240/404] Loss: 0.0030\n",
      "Epoch [22/30], Iter [250/404] Loss: 0.0091\n",
      "Epoch [22/30], Iter [260/404] Loss: 0.0106\n",
      "Epoch [22/30], Iter [270/404] Loss: 0.0549\n",
      "Epoch [22/30], Iter [280/404] Loss: 0.0012\n",
      "Epoch [22/30], Iter [290/404] Loss: 0.0016\n",
      "Epoch [22/30], Iter [300/404] Loss: 0.0741\n",
      "Epoch [22/30], Iter [310/404] Loss: 0.0591\n",
      "Epoch [22/30], Iter [320/404] Loss: 0.0411\n",
      "Epoch [22/30], Iter [330/404] Loss: 0.0788\n",
      "Epoch [22/30], Iter [340/404] Loss: 0.0093\n",
      "Epoch [22/30], Iter [350/404] Loss: 0.0714\n",
      "Epoch [22/30], Iter [360/404] Loss: 0.0051\n",
      "Epoch [22/30], Iter [370/404] Loss: 0.0013\n",
      "Epoch [22/30], Iter [380/404] Loss: 0.0009\n",
      "Epoch [22/30], Iter [390/404] Loss: 0.0022\n",
      "Epoch [22/30], Iter [400/404] Loss: 0.0083\n",
      "Epoch [23/30], Iter [10/404] Loss: 0.0327\n",
      "Epoch [23/30], Iter [20/404] Loss: 0.0462\n",
      "Epoch [23/30], Iter [30/404] Loss: 0.0177\n",
      "Epoch [23/30], Iter [40/404] Loss: 0.0055\n",
      "Epoch [23/30], Iter [50/404] Loss: 0.0094\n",
      "Epoch [23/30], Iter [60/404] Loss: 0.1290\n",
      "Epoch [23/30], Iter [70/404] Loss: 0.0196\n",
      "Epoch [23/30], Iter [80/404] Loss: 0.0003\n",
      "Epoch [23/30], Iter [90/404] Loss: 0.0018\n",
      "Epoch [23/30], Iter [100/404] Loss: 0.0004\n",
      "Epoch [23/30], Iter [110/404] Loss: 0.0583\n",
      "Epoch [23/30], Iter [120/404] Loss: 0.0205\n",
      "Epoch [23/30], Iter [130/404] Loss: 0.0201\n",
      "Epoch [23/30], Iter [140/404] Loss: 0.0978\n",
      "Epoch [23/30], Iter [150/404] Loss: 0.0825\n",
      "Epoch [23/30], Iter [160/404] Loss: 0.1745\n",
      "Epoch [23/30], Iter [170/404] Loss: 0.0416\n",
      "Epoch [23/30], Iter [180/404] Loss: 0.0570\n",
      "Epoch [23/30], Iter [190/404] Loss: 0.2807\n",
      "Epoch [23/30], Iter [200/404] Loss: 0.0009\n",
      "Epoch [23/30], Iter [210/404] Loss: 0.0614\n",
      "Epoch [23/30], Iter [220/404] Loss: 0.0002\n",
      "Epoch [23/30], Iter [230/404] Loss: 0.0007\n",
      "Epoch [23/30], Iter [240/404] Loss: 0.0052\n",
      "Epoch [23/30], Iter [250/404] Loss: 0.0664\n",
      "Epoch [23/30], Iter [260/404] Loss: 0.1837\n",
      "Epoch [23/30], Iter [270/404] Loss: 0.0635\n",
      "Epoch [23/30], Iter [280/404] Loss: 0.0195\n",
      "Epoch [23/30], Iter [290/404] Loss: 0.0743\n",
      "Epoch [23/30], Iter [300/404] Loss: 0.0180\n",
      "Epoch [23/30], Iter [310/404] Loss: 0.0048\n",
      "Epoch [23/30], Iter [320/404] Loss: 0.0002\n",
      "Epoch [23/30], Iter [330/404] Loss: 0.0532\n",
      "Epoch [23/30], Iter [340/404] Loss: 0.0002\n",
      "Epoch [23/30], Iter [350/404] Loss: 0.0171\n",
      "Epoch [23/30], Iter [360/404] Loss: 0.0788\n",
      "Epoch [23/30], Iter [370/404] Loss: 0.0061\n",
      "Epoch [23/30], Iter [380/404] Loss: 0.0348\n",
      "Epoch [23/30], Iter [390/404] Loss: 0.0288\n",
      "Epoch [23/30], Iter [400/404] Loss: 0.0402\n",
      "Epoch [24/30], Iter [10/404] Loss: 0.2016\n",
      "Epoch [24/30], Iter [20/404] Loss: 0.0161\n",
      "Epoch [24/30], Iter [30/404] Loss: 0.0068\n",
      "Epoch [24/30], Iter [40/404] Loss: 0.0857\n",
      "Epoch [24/30], Iter [50/404] Loss: 0.0028\n",
      "Epoch [24/30], Iter [60/404] Loss: 0.0187\n",
      "Epoch [24/30], Iter [70/404] Loss: 0.0074\n",
      "Epoch [24/30], Iter [80/404] Loss: 0.0081\n",
      "Epoch [24/30], Iter [90/404] Loss: 0.2738\n",
      "Epoch [24/30], Iter [100/404] Loss: 0.0007\n",
      "Epoch [24/30], Iter [110/404] Loss: 0.0007\n",
      "Epoch [24/30], Iter [120/404] Loss: 0.1428\n",
      "Epoch [24/30], Iter [130/404] Loss: 0.0057\n",
      "Epoch [24/30], Iter [140/404] Loss: 0.0655\n",
      "Epoch [24/30], Iter [150/404] Loss: 0.0254\n",
      "Epoch [24/30], Iter [160/404] Loss: 0.0045\n",
      "Epoch [24/30], Iter [170/404] Loss: 0.0026\n",
      "Epoch [24/30], Iter [180/404] Loss: 0.0041\n",
      "Epoch [24/30], Iter [190/404] Loss: 0.0547\n",
      "Epoch [24/30], Iter [200/404] Loss: 0.0028\n",
      "Epoch [24/30], Iter [210/404] Loss: 0.0025\n",
      "Epoch [24/30], Iter [220/404] Loss: 0.1853\n",
      "Epoch [24/30], Iter [230/404] Loss: 0.0000\n",
      "Epoch [24/30], Iter [240/404] Loss: 0.0243\n",
      "Epoch [24/30], Iter [250/404] Loss: 0.0001\n",
      "Epoch [24/30], Iter [260/404] Loss: 0.0280\n",
      "Epoch [24/30], Iter [270/404] Loss: 0.0732\n",
      "Epoch [24/30], Iter [280/404] Loss: 0.0004\n",
      "Epoch [24/30], Iter [290/404] Loss: 0.1078\n",
      "Epoch [24/30], Iter [300/404] Loss: 0.0055\n",
      "Epoch [24/30], Iter [310/404] Loss: 0.0061\n",
      "Epoch [24/30], Iter [320/404] Loss: 0.0112\n",
      "Epoch [24/30], Iter [330/404] Loss: 0.0848\n",
      "Epoch [24/30], Iter [340/404] Loss: 0.0034\n",
      "Epoch [24/30], Iter [350/404] Loss: 0.0037\n",
      "Epoch [24/30], Iter [360/404] Loss: 0.0842\n",
      "Epoch [24/30], Iter [370/404] Loss: 0.0027\n",
      "Epoch [24/30], Iter [380/404] Loss: 0.0013\n",
      "Epoch [24/30], Iter [390/404] Loss: 0.0813\n",
      "Epoch [24/30], Iter [400/404] Loss: 0.0051\n",
      "Epoch [25/30], Iter [10/404] Loss: 0.0222\n",
      "Epoch [25/30], Iter [20/404] Loss: 0.0859\n",
      "Epoch [25/30], Iter [30/404] Loss: 0.0116\n",
      "Epoch [25/30], Iter [40/404] Loss: 0.2486\n",
      "Epoch [25/30], Iter [50/404] Loss: 0.0030\n",
      "Epoch [25/30], Iter [60/404] Loss: 0.0122\n",
      "Epoch [25/30], Iter [70/404] Loss: 0.0250\n",
      "Epoch [25/30], Iter [80/404] Loss: 0.0060\n",
      "Epoch [25/30], Iter [90/404] Loss: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Iter [100/404] Loss: 0.0011\n",
      "Epoch [25/30], Iter [110/404] Loss: 0.0624\n",
      "Epoch [25/30], Iter [120/404] Loss: 0.0010\n",
      "Epoch [25/30], Iter [130/404] Loss: 0.0064\n",
      "Epoch [25/30], Iter [140/404] Loss: 0.0817\n",
      "Epoch [25/30], Iter [150/404] Loss: 0.0118\n",
      "Epoch [25/30], Iter [160/404] Loss: 0.0010\n",
      "Epoch [25/30], Iter [170/404] Loss: 0.0600\n",
      "Epoch [25/30], Iter [180/404] Loss: 0.0111\n",
      "Epoch [25/30], Iter [190/404] Loss: 0.0050\n",
      "Epoch [25/30], Iter [200/404] Loss: 0.0351\n",
      "Epoch [25/30], Iter [210/404] Loss: 0.0033\n",
      "Epoch [25/30], Iter [220/404] Loss: 0.0000\n",
      "Epoch [25/30], Iter [230/404] Loss: 0.0912\n",
      "Epoch [25/30], Iter [240/404] Loss: 0.0259\n",
      "Epoch [25/30], Iter [250/404] Loss: 0.0000\n",
      "Epoch [25/30], Iter [260/404] Loss: 0.0011\n",
      "Epoch [25/30], Iter [270/404] Loss: 0.1058\n",
      "Epoch [25/30], Iter [280/404] Loss: 0.0020\n",
      "Epoch [25/30], Iter [290/404] Loss: 0.0159\n",
      "Epoch [25/30], Iter [300/404] Loss: 0.1588\n",
      "Epoch [25/30], Iter [310/404] Loss: 0.0001\n",
      "Epoch [25/30], Iter [320/404] Loss: 0.0001\n",
      "Epoch [25/30], Iter [330/404] Loss: 0.0033\n",
      "Epoch [25/30], Iter [340/404] Loss: 0.0916\n",
      "Epoch [25/30], Iter [350/404] Loss: 0.2782\n",
      "Epoch [25/30], Iter [360/404] Loss: 0.0721\n",
      "Epoch [25/30], Iter [370/404] Loss: 0.0279\n",
      "Epoch [25/30], Iter [380/404] Loss: 0.0120\n",
      "Epoch [25/30], Iter [390/404] Loss: 0.0085\n",
      "Epoch [25/30], Iter [400/404] Loss: 0.0662\n",
      "Epoch [26/30], Iter [10/404] Loss: 0.0880\n",
      "Epoch [26/30], Iter [20/404] Loss: 0.0161\n",
      "Epoch [26/30], Iter [30/404] Loss: 0.0339\n",
      "Epoch [26/30], Iter [40/404] Loss: 0.0071\n",
      "Epoch [26/30], Iter [50/404] Loss: 0.0094\n",
      "Epoch [26/30], Iter [60/404] Loss: 0.0042\n",
      "Epoch [26/30], Iter [70/404] Loss: 0.0788\n",
      "Epoch [26/30], Iter [80/404] Loss: 0.0090\n",
      "Epoch [26/30], Iter [90/404] Loss: 0.0000\n",
      "Epoch [26/30], Iter [100/404] Loss: 0.0003\n",
      "Epoch [26/30], Iter [110/404] Loss: 0.0824\n",
      "Epoch [26/30], Iter [120/404] Loss: 0.0142\n",
      "Epoch [26/30], Iter [130/404] Loss: 0.0220\n",
      "Epoch [26/30], Iter [140/404] Loss: 0.1585\n",
      "Epoch [26/30], Iter [150/404] Loss: 0.0080\n",
      "Epoch [26/30], Iter [160/404] Loss: 0.1260\n",
      "Epoch [26/30], Iter [170/404] Loss: 0.0002\n",
      "Epoch [26/30], Iter [180/404] Loss: 0.0006\n",
      "Epoch [26/30], Iter [190/404] Loss: 0.0068\n",
      "Epoch [26/30], Iter [200/404] Loss: 0.0046\n",
      "Epoch [26/30], Iter [210/404] Loss: 0.0001\n",
      "Epoch [26/30], Iter [220/404] Loss: 0.0021\n",
      "Epoch [26/30], Iter [230/404] Loss: 0.0832\n",
      "Epoch [26/30], Iter [240/404] Loss: 0.0660\n",
      "Epoch [26/30], Iter [250/404] Loss: 0.0807\n",
      "Epoch [26/30], Iter [260/404] Loss: 0.0013\n",
      "Epoch [26/30], Iter [270/404] Loss: 0.0024\n",
      "Epoch [26/30], Iter [280/404] Loss: 0.0125\n",
      "Epoch [26/30], Iter [290/404] Loss: 0.0215\n",
      "Epoch [26/30], Iter [300/404] Loss: 0.0159\n",
      "Epoch [26/30], Iter [310/404] Loss: 0.1327\n",
      "Epoch [26/30], Iter [320/404] Loss: 0.0018\n",
      "Epoch [26/30], Iter [330/404] Loss: 0.0001\n",
      "Epoch [26/30], Iter [340/404] Loss: 0.0014\n",
      "Epoch [26/30], Iter [350/404] Loss: 0.0230\n",
      "Epoch [26/30], Iter [360/404] Loss: 0.0810\n",
      "Epoch [26/30], Iter [370/404] Loss: 0.0011\n",
      "Epoch [26/30], Iter [380/404] Loss: 0.1179\n",
      "Epoch [26/30], Iter [390/404] Loss: 0.0193\n",
      "Epoch [26/30], Iter [400/404] Loss: 0.0074\n",
      "Epoch [27/30], Iter [10/404] Loss: 0.0001\n",
      "Epoch [27/30], Iter [20/404] Loss: 0.0052\n",
      "Epoch [27/30], Iter [30/404] Loss: 0.0005\n",
      "Epoch [27/30], Iter [40/404] Loss: 0.0669\n",
      "Epoch [27/30], Iter [50/404] Loss: 0.0004\n",
      "Epoch [27/30], Iter [60/404] Loss: 0.0202\n",
      "Epoch [27/30], Iter [70/404] Loss: 0.0000\n",
      "Epoch [27/30], Iter [80/404] Loss: 0.0001\n",
      "Epoch [27/30], Iter [90/404] Loss: 0.0006\n",
      "Epoch [27/30], Iter [100/404] Loss: 0.0004\n",
      "Epoch [27/30], Iter [110/404] Loss: 0.2262\n",
      "Epoch [27/30], Iter [120/404] Loss: 0.0364\n",
      "Epoch [27/30], Iter [130/404] Loss: 0.0010\n",
      "Epoch [27/30], Iter [140/404] Loss: 0.0239\n",
      "Epoch [27/30], Iter [150/404] Loss: 0.0129\n",
      "Epoch [27/30], Iter [160/404] Loss: 0.0549\n",
      "Epoch [27/30], Iter [170/404] Loss: 0.0003\n",
      "Epoch [27/30], Iter [180/404] Loss: 0.0180\n",
      "Epoch [27/30], Iter [190/404] Loss: 0.0006\n",
      "Epoch [27/30], Iter [200/404] Loss: 0.0012\n",
      "Epoch [27/30], Iter [210/404] Loss: 0.0000\n",
      "Epoch [27/30], Iter [220/404] Loss: 0.1291\n",
      "Epoch [27/30], Iter [230/404] Loss: 0.0004\n",
      "Epoch [27/30], Iter [240/404] Loss: 0.0005\n",
      "Epoch [27/30], Iter [250/404] Loss: 0.0001\n",
      "Epoch [27/30], Iter [260/404] Loss: 0.0636\n",
      "Epoch [27/30], Iter [270/404] Loss: 0.0000\n",
      "Epoch [27/30], Iter [280/404] Loss: 0.0219\n",
      "Epoch [27/30], Iter [290/404] Loss: 0.0005\n",
      "Epoch [27/30], Iter [300/404] Loss: 0.0994\n",
      "Epoch [27/30], Iter [310/404] Loss: 0.0002\n",
      "Epoch [27/30], Iter [320/404] Loss: 0.0195\n",
      "Epoch [27/30], Iter [330/404] Loss: 0.0457\n",
      "Epoch [27/30], Iter [340/404] Loss: 0.0909\n",
      "Epoch [27/30], Iter [350/404] Loss: 0.0054\n",
      "Epoch [27/30], Iter [360/404] Loss: 0.0183\n",
      "Epoch [27/30], Iter [370/404] Loss: 0.0540\n",
      "Epoch [27/30], Iter [380/404] Loss: 0.0513\n",
      "Epoch [27/30], Iter [390/404] Loss: 0.0005\n",
      "Epoch [27/30], Iter [400/404] Loss: 0.0002\n",
      "Epoch [28/30], Iter [10/404] Loss: 0.0032\n",
      "Epoch [28/30], Iter [20/404] Loss: 0.0073\n",
      "Epoch [28/30], Iter [30/404] Loss: 0.0004\n",
      "Epoch [28/30], Iter [40/404] Loss: 0.0344\n",
      "Epoch [28/30], Iter [50/404] Loss: 0.0000\n",
      "Epoch [28/30], Iter [60/404] Loss: 0.0714\n",
      "Epoch [28/30], Iter [70/404] Loss: 0.0297\n",
      "Epoch [28/30], Iter [80/404] Loss: 0.0012\n",
      "Epoch [28/30], Iter [90/404] Loss: 0.0263\n",
      "Epoch [28/30], Iter [100/404] Loss: 0.0091\n",
      "Epoch [28/30], Iter [110/404] Loss: 0.0077\n",
      "Epoch [28/30], Iter [120/404] Loss: 0.0004\n",
      "Epoch [28/30], Iter [130/404] Loss: 0.2002\n",
      "Epoch [28/30], Iter [140/404] Loss: 0.0000\n",
      "Epoch [28/30], Iter [150/404] Loss: 0.0232\n",
      "Epoch [28/30], Iter [160/404] Loss: 0.0886\n",
      "Epoch [28/30], Iter [170/404] Loss: 0.0168\n",
      "Epoch [28/30], Iter [180/404] Loss: 0.0995\n",
      "Epoch [28/30], Iter [190/404] Loss: 0.0000\n",
      "Epoch [28/30], Iter [200/404] Loss: 0.0002\n",
      "Epoch [28/30], Iter [210/404] Loss: 0.0086\n",
      "Epoch [28/30], Iter [220/404] Loss: 0.0640\n",
      "Epoch [28/30], Iter [230/404] Loss: 0.0080\n",
      "Epoch [28/30], Iter [240/404] Loss: 0.0001\n",
      "Epoch [28/30], Iter [250/404] Loss: 0.0454\n",
      "Epoch [28/30], Iter [260/404] Loss: 0.0001\n",
      "Epoch [28/30], Iter [270/404] Loss: 0.0003\n",
      "Epoch [28/30], Iter [280/404] Loss: 0.0002\n",
      "Epoch [28/30], Iter [290/404] Loss: 0.0409\n",
      "Epoch [28/30], Iter [300/404] Loss: 0.0036\n",
      "Epoch [28/30], Iter [310/404] Loss: 0.0612\n",
      "Epoch [28/30], Iter [320/404] Loss: 0.0028\n",
      "Epoch [28/30], Iter [330/404] Loss: 0.0022\n",
      "Epoch [28/30], Iter [340/404] Loss: 0.0962\n",
      "Epoch [28/30], Iter [350/404] Loss: 0.0000\n",
      "Epoch [28/30], Iter [360/404] Loss: 0.0058\n",
      "Epoch [28/30], Iter [370/404] Loss: 0.0899\n",
      "Epoch [28/30], Iter [380/404] Loss: 0.0051\n",
      "Epoch [28/30], Iter [390/404] Loss: 0.0090\n",
      "Epoch [28/30], Iter [400/404] Loss: 0.0492\n",
      "Epoch [29/30], Iter [10/404] Loss: 0.0056\n",
      "Epoch [29/30], Iter [20/404] Loss: 0.0416\n",
      "Epoch [29/30], Iter [30/404] Loss: 0.0037\n",
      "Epoch [29/30], Iter [40/404] Loss: 0.0022\n",
      "Epoch [29/30], Iter [50/404] Loss: 0.0029\n",
      "Epoch [29/30], Iter [60/404] Loss: 0.0002\n",
      "Epoch [29/30], Iter [70/404] Loss: 0.0673\n",
      "Epoch [29/30], Iter [80/404] Loss: 0.0008\n",
      "Epoch [29/30], Iter [90/404] Loss: 0.0104\n",
      "Epoch [29/30], Iter [100/404] Loss: 0.0085\n",
      "Epoch [29/30], Iter [110/404] Loss: 0.0016\n",
      "Epoch [29/30], Iter [120/404] Loss: 0.0007\n",
      "Epoch [29/30], Iter [130/404] Loss: 0.0207\n",
      "Epoch [29/30], Iter [140/404] Loss: 0.0119\n",
      "Epoch [29/30], Iter [150/404] Loss: 0.0472\n",
      "Epoch [29/30], Iter [160/404] Loss: 0.0002\n",
      "Epoch [29/30], Iter [170/404] Loss: 0.0009\n",
      "Epoch [29/30], Iter [180/404] Loss: 0.0001\n",
      "Epoch [29/30], Iter [190/404] Loss: 0.0001\n",
      "Epoch [29/30], Iter [200/404] Loss: 0.0056\n",
      "Epoch [29/30], Iter [210/404] Loss: 0.0474\n",
      "Epoch [29/30], Iter [220/404] Loss: 0.0464\n",
      "Epoch [29/30], Iter [230/404] Loss: 0.0005\n",
      "Epoch [29/30], Iter [240/404] Loss: 0.0796\n",
      "Epoch [29/30], Iter [250/404] Loss: 0.2068\n",
      "Epoch [29/30], Iter [260/404] Loss: 0.0005\n",
      "Epoch [29/30], Iter [270/404] Loss: 0.0005\n",
      "Epoch [29/30], Iter [280/404] Loss: 0.0012\n",
      "Epoch [29/30], Iter [290/404] Loss: 0.0012\n",
      "Epoch [29/30], Iter [300/404] Loss: 0.0000\n",
      "Epoch [29/30], Iter [310/404] Loss: 0.0111\n",
      "Epoch [29/30], Iter [320/404] Loss: 0.0002\n",
      "Epoch [29/30], Iter [330/404] Loss: 0.0613\n",
      "Epoch [29/30], Iter [340/404] Loss: 0.0000\n",
      "Epoch [29/30], Iter [350/404] Loss: 0.0000\n",
      "Epoch [29/30], Iter [360/404] Loss: 0.0000\n",
      "Epoch [29/30], Iter [370/404] Loss: 0.0014\n",
      "Epoch [29/30], Iter [380/404] Loss: 0.0008\n",
      "Epoch [29/30], Iter [390/404] Loss: 0.0014\n",
      "Epoch [29/30], Iter [400/404] Loss: 0.0513\n",
      "Epoch [30/30], Iter [10/404] Loss: 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Iter [20/404] Loss: 0.0005\n",
      "Epoch [30/30], Iter [30/404] Loss: 0.0013\n",
      "Epoch [30/30], Iter [40/404] Loss: 0.0031\n",
      "Epoch [30/30], Iter [50/404] Loss: 0.0016\n",
      "Epoch [30/30], Iter [60/404] Loss: 0.0012\n",
      "Epoch [30/30], Iter [70/404] Loss: 0.0440\n",
      "Epoch [30/30], Iter [80/404] Loss: 0.0006\n",
      "Epoch [30/30], Iter [90/404] Loss: 0.0516\n",
      "Epoch [30/30], Iter [100/404] Loss: 0.0003\n",
      "Epoch [30/30], Iter [110/404] Loss: 0.0008\n",
      "Epoch [30/30], Iter [120/404] Loss: 0.0663\n",
      "Epoch [30/30], Iter [130/404] Loss: 0.0007\n",
      "Epoch [30/30], Iter [140/404] Loss: 0.0007\n",
      "Epoch [30/30], Iter [150/404] Loss: 0.0788\n",
      "Epoch [30/30], Iter [160/404] Loss: 0.0622\n",
      "Epoch [30/30], Iter [170/404] Loss: 0.0134\n",
      "Epoch [30/30], Iter [180/404] Loss: 0.0600\n",
      "Epoch [30/30], Iter [190/404] Loss: 0.0047\n",
      "Epoch [30/30], Iter [200/404] Loss: 0.0007\n",
      "Epoch [30/30], Iter [210/404] Loss: 0.0775\n",
      "Epoch [30/30], Iter [220/404] Loss: 0.0001\n",
      "Epoch [30/30], Iter [230/404] Loss: 0.0012\n",
      "Epoch [30/30], Iter [240/404] Loss: 0.0040\n",
      "Epoch [30/30], Iter [250/404] Loss: 0.0000\n",
      "Epoch [30/30], Iter [260/404] Loss: 0.0694\n",
      "Epoch [30/30], Iter [270/404] Loss: 0.0001\n",
      "Epoch [30/30], Iter [280/404] Loss: 0.0123\n",
      "Epoch [30/30], Iter [290/404] Loss: 0.0399\n",
      "Epoch [30/30], Iter [300/404] Loss: 0.0010\n",
      "Epoch [30/30], Iter [310/404] Loss: 0.0262\n",
      "Epoch [30/30], Iter [320/404] Loss: 0.0031\n",
      "Epoch [30/30], Iter [330/404] Loss: 0.0064\n",
      "Epoch [30/30], Iter [340/404] Loss: 0.0020\n",
      "Epoch [30/30], Iter [350/404] Loss: 0.0020\n",
      "Epoch [30/30], Iter [360/404] Loss: 0.0036\n",
      "Epoch [30/30], Iter [370/404] Loss: 0.0016\n",
      "Epoch [30/30], Iter [380/404] Loss: 0.1422\n",
      "Epoch [30/30], Iter [390/404] Loss: 0.0001\n",
      "Epoch [30/30], Iter [400/404] Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr = learning_rate, momentum=momentum)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n",
    "                %(epoch+1, num_epochs, i+1, len(trainset)//batch_size, loss.data[0]))\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn-vol-2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing neural network on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 98 %\n",
      "Time for running: 125\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "cnn = CNN(n_classes)\n",
    "cnn.load_state_dict(torch.load('cnn-vol-2.pt'))\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('Test Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n",
    "print('Time for running: %d' % (elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN trained on 16K cropped images and tested on 5K images. Dataset created from 4332 distinct(unique) images. Then, shifted by x1, x2 and so on. Generated new dataset has 21K images. CNN trained on 30 batched dataset with 0.001 learning rate and 0.9 momentum. Epochs count = 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to show plot with epoch num and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e792956d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_file = 'cnn-vol-2-trained-losses.txt' \n",
    "def read_from_loss(file):\n",
    "    with open (file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.split(' ') for x in content]\n",
    "        content = [[x[i].split('\\n')[0] for i in range(len(x)) if i%2!=0] for x in content]\n",
    "        content = [[x[0]+x[1] , x[2]] for x in content]\n",
    "    data = []\n",
    "    for item in content:\n",
    "        tmp = item[0].split(',')\n",
    "        num = \"\"\n",
    "        for t in tmp:\n",
    "            num += (t.split('/')[0].split('[')[1]) + '.'\n",
    "        iteration = float(num[:-1])\n",
    "        data.append([iteration, float(item[1])])\n",
    "    data = np.array(data)\n",
    "    plt.plot(data[: , 0], data[: , 1])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "read_from_loss(loss_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing FlickrLogos - 32 Dataset for train/test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_DIR = '../data/fl32/original'\n",
    "CROPPED_DATA_DIR = '../data/fl32/images'\n",
    "ORIGINAL_ANNOTATION = '../data/fl32/annotation.txt'\n",
    "CROPPED_ANNOTATION = '../data/fl32/crop_annotation.txt'\n",
    "\n",
    "TRAIN_SET = '../annotations/trainset-32.txt'\n",
    "TEST_SET = '../annotations/testset-32.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing labels with new class - *'no logo'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = prepare_labels(ORIGINAL_ANNOTATION)\n",
    "LABELS.append('nologo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating resized crop logos from FL32 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_resized_crops(ORIGINAL_ANNOTATION, ORIGINAL_DATA_DIR, CROPPED_ANNOTATION, CROPPED_DATA_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
